{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade kagglehub[pandas-datasets,hf-datasets]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:32:11.528689Z","iopub.execute_input":"2025-05-07T09:32:11.529001Z","iopub.status.idle":"2025-05-07T09:32:16.824322Z","shell.execute_reply.started":"2025-05-07T09:32:11.528921Z","shell.execute_reply":"2025-05-07T09:32:16.823464Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: kagglehub[hf-datasets,pandas-datasets] in /usr/local/lib/python3.11/dist-packages (0.3.11)\nCollecting kagglehub[hf-datasets,pandas-datasets]\n  Downloading kagglehub-0.3.12-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub[hf-datasets,pandas-datasets]) (24.2)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub[hf-datasets,pandas-datasets]) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub[hf-datasets,pandas-datasets]) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub[hf-datasets,pandas-datasets]) (4.67.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from kagglehub[hf-datasets,pandas-datasets]) (3.5.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from kagglehub[hf-datasets,pandas-datasets]) (2.2.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->kagglehub[hf-datasets,pandas-datasets])\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (3.11.16)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (0.30.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[hf-datasets,pandas-datasets]) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[hf-datasets,pandas-datasets]) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[hf-datasets,pandas-datasets]) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[hf-datasets,pandas-datasets]) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->kagglehub[hf-datasets,pandas-datasets]) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->kagglehub[hf-datasets,pandas-datasets]) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->kagglehub[hf-datasets,pandas-datasets]) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->kagglehub[hf-datasets,pandas-datasets]) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->kagglehub[hf-datasets,pandas-datasets]) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->kagglehub[hf-datasets,pandas-datasets]) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->kagglehub[hf-datasets,pandas-datasets]) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->kagglehub[hf-datasets,pandas-datasets]) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->kagglehub[hf-datasets,pandas-datasets]) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->kagglehub[hf-datasets,pandas-datasets]) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets->kagglehub[hf-datasets,pandas-datasets]) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->kagglehub[hf-datasets,pandas-datasets]) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (2024.2.0)\nDownloading kagglehub-0.3.12-py3-none-any.whl (67 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, kagglehub\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: kagglehub\n    Found existing installation: kagglehub 0.3.11\n    Uninstalling kagglehub-0.3.11:\n      Successfully uninstalled kagglehub-0.3.11\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2024.12.0 kagglehub-0.3.12\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!apt-get install git -y\n\n!git clone https://github.com/miriam-16/aml_1_aerial_imagery.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:32:16.825664Z","iopub.execute_input":"2025-05-07T09:32:16.825963Z","iopub.status.idle":"2025-05-07T09:32:19.535697Z","shell.execute_reply.started":"2025-05-07T09:32:16.825931Z","shell.execute_reply":"2025-05-07T09:32:19.534902Z"}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ngit is already the newest version (1:2.34.1-1ubuntu1.12).\n0 upgraded, 0 newly installed, 0 to remove and 122 not upgraded.\nfatal: destination path 'aml_1_aerial_imagery' already exists and is not an empty directory.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nimport copy\nimport os\nimport torch\nfrom PIL import Image\nfrom PIL import Image, ImageDraw\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nfrom torch.utils.data import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch.nn as nn\nfrom torchvision import utils\n%matplotlib inline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:32:19.536810Z","iopub.execute_input":"2025-05-07T09:32:19.537149Z","iopub.status.idle":"2025-05-07T09:32:22.880827Z","shell.execute_reply.started":"2025-05-07T09:32:19.537091Z","shell.execute_reply":"2025-05-07T09:32:22.880167Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"path = \"/kaggle/working/aml_1_aerial_imagery/dataset\"\nimport pandas as pd\n\nlabels_df = pd.read_csv(path+'/train.csv')  # Adjust filename\nprint(labels_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:32:22.882939Z","iopub.execute_input":"2025-05-07T09:32:22.883468Z","iopub.status.idle":"2025-05-07T09:32:22.911770Z","shell.execute_reply.started":"2025-05-07T09:32:22.883443Z","shell.execute_reply":"2025-05-07T09:32:22.910816Z"}},"outputs":[{"name":"stdout","text":"                                     id  has_cactus\n0  0004be2cfeaba1c0361d39e2b000257b.jpg           1\n1  000c8a36845c0208e833c79c1bffedd1.jpg           1\n2  000d1e9a533f62e55c289303b072733d.jpg           1\n3  0011485b40695e9138e92d0b3fb55128.jpg           1\n4  0014d7a11e90b62848904c1418fc8cf2.jpg           1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(labels_df.shape)\nlabels_df[labels_df.duplicated(keep=False)]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:32:22.912653Z","iopub.execute_input":"2025-05-07T09:32:22.912978Z","iopub.status.idle":"2025-05-07T09:32:22.926255Z","shell.execute_reply.started":"2025-05-07T09:32:22.912950Z","shell.execute_reply":"2025-05-07T09:32:22.925436Z"}},"outputs":[{"name":"stdout","text":"(17500, 2)\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [id, has_cactus]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>has_cactus</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"labels_df['has_cactus'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:32:22.927024Z","iopub.execute_input":"2025-05-07T09:32:22.927281Z","iopub.status.idle":"2025-05-07T09:32:22.937016Z","shell.execute_reply.started":"2025-05-07T09:32:22.927254Z","shell.execute_reply":"2025-05-07T09:32:22.936305Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"has_cactus\n1    13136\n0     4364\nName: count, dtype: int64"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import torch\ntorch.manual_seed(0)\n\nfrom torch.utils.data import Dataset\nimport os\nimport pandas as pd\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\nclass pytorch_data(Dataset):\n    \n    def __init__(self, data_dir, transform, data_type=\"train\"):\n        # Get Image File Names\n        cdm_data = os.path.join(data_dir, data_type)\n        file_names = os.listdir(cdm_data)\n\n        all_image_paths = [os.path.join(cdm_data, f) for f in file_names if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n\n        print(f\"Found {len(all_image_paths)} images in directory.\")\n        print(f\"Sample filenames: {all_image_paths[:1]}\")\n\n        # Get Labels\n        labels_data = os.path.join(\"/kaggle/working/aml_1_aerial_imagery/dataset/\", \"train.csv\")\n        labels_df = pd.read_csv(labels_data)\n\n        # Normalize index: remove extensions if present\n        labels_df['id'] = labels_df['id'].apply(lambda x: os.path.splitext(str(x))[0])\n        labels_df.set_index(\"id\", inplace=True)\n\n        print(f\"Labels dataframe length: {len(labels_df)}\")\n\n        # Extract only valid images (that have a label)\n        valid_filenames = []\n        labels = []\n\n        for f in all_image_paths:\n            filename = os.path.basename(f)  # get only file name\n            image_id = os.path.splitext(filename)[0]  # remove extension\n\n            if image_id in labels_df.index:\n                valid_filenames.append(f)\n                labels.append(labels_df.loc[image_id].values[0])\n            else:\n                print(f\"Warning: image '{filename}' has no matching label in train.csv\")\n\n        self.full_filenames = valid_filenames\n        self.labels = labels\n        self.transform = transform\n\n        print(f\"Valid image-label pairs: {len(self.full_filenames)}\")\n        print(f\"First few labels: {self.labels[:5]}\")\n      \n    def __len__(self):\n        return len(self.full_filenames)\n      \n    def __getitem__(self, idx):\n        if idx >= len(self.full_filenames):\n            raise IndexError(f\"Index {idx} out of bounds for dataset of length {len(self.full_filenames)}\")\n\n        image = Image.open(self.full_filenames[idx])\n        image = self.transform(image)\n        return image, self.labels[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:32:22.937796Z","iopub.execute_input":"2025-05-07T09:32:22.938033Z","iopub.status.idle":"2025-05-07T09:32:22.955879Z","shell.execute_reply.started":"2025-05-07T09:32:22.938013Z","shell.execute_reply":"2025-05-07T09:32:22.955164Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# define transformation that converts a PIL image into PyTorch tensors\nimport torchvision.transforms as transforms\ndata_transformer = transforms.Compose([transforms.ToTensor(),\n                                       transforms.Resize((32,32))])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:32:22.956695Z","iopub.execute_input":"2025-05-07T09:32:22.956986Z","iopub.status.idle":"2025-05-07T09:32:22.974688Z","shell.execute_reply.started":"2025-05-07T09:32:22.956959Z","shell.execute_reply":"2025-05-07T09:32:22.973940Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Define an object of the custom dataset for the train folder.\ndata_dir = path+'/train/'\nimg_dataset = pytorch_data(data_dir, data_transformer, \"train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:32:22.975469Z","iopub.execute_input":"2025-05-07T09:32:22.975692Z","iopub.status.idle":"2025-05-07T09:32:23.481590Z","shell.execute_reply.started":"2025-05-07T09:32:22.975674Z","shell.execute_reply":"2025-05-07T09:32:23.480734Z"}},"outputs":[{"name":"stdout","text":"Found 17500 images in directory.\nSample filenames: ['/kaggle/working/aml_1_aerial_imagery/dataset/train/train/060a884c1005baed62bd6c4a72220da2.jpg']\nLabels dataframe length: 17500\nValid image-label pairs: 17500\nFirst few labels: [0, 0, 0, 0, 1]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# define transformation that converts a PIL image into PyTorch tensors\ndata_transformer = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((32, 32))\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:32:23.483440Z","iopub.execute_input":"2025-05-07T09:32:23.483653Z","iopub.status.idle":"2025-05-07T09:32:23.487749Z","shell.execute_reply.started":"2025-05-07T09:32:23.483636Z","shell.execute_reply":"2025-05-07T09:32:23.486730Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Test a sample\nimg, label = img_dataset[10]\nprint(img.shape, torch.min(img), torch.max(img))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:32:23.488397Z","iopub.execute_input":"2025-05-07T09:32:23.488625Z","iopub.status.idle":"2025-05-07T09:32:23.511845Z","shell.execute_reply.started":"2025-05-07T09:32:23.488608Z","shell.execute_reply":"2025-05-07T09:32:23.510886Z"}},"outputs":[{"name":"stdout","text":"torch.Size([3, 32, 32]) tensor(0.1294) tensor(0.9412)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Aumentiamo le immagini della classe 0\nfrom torchvision.transforms import RandomRotation, ToTensor, Resize\nfrom tqdm import tqdm\n\n# Trasformazione per augmentare\naugment_transform = transforms.Compose([\n    RandomRotation(degrees=10),\n    Resize((32, 32)),\n    ToTensor()\n])\n\n# Trova solo immagini con etichetta 0\nimages_class0 = [i for i in range(len(img_dataset)) if img_dataset.labels[i] == 0]\n\n# Duplichiamo queste immagini con trasformazione\naugmented_images = []\naugmented_labels = []\n\nfor idx in tqdm(images_class0):\n    img_path = img_dataset.full_filenames[idx]\n    img = Image.open(img_path)\n    augmented_img = augment_transform(img)\n    augmented_images.append(augmented_img)\n    augmented_labels.append(0)\n\n# Stack immagini originali\noriginal_images = [img_dataset[i][0] for i in range(len(img_dataset))]\noriginal_labels = [img_dataset[i][1] for i in range(len(img_dataset))]\n\n# Combina immagini originali + augmentate\nall_images = torch.stack(original_images + augmented_images)\nall_labels = torch.tensor(original_labels + augmented_labels)\n\n# Nuovo Dataset custom con dati augmentati\nclass AugmentedDataset(torch.utils.data.Dataset):\n    def __init__(self, images, labels):\n        self.images = images\n        self.labels = labels\n    def __len__(self):\n        return len(self.images)\n    def __getitem__(self, idx):\n        return self.images[idx], self.labels[idx]\n\n# Sostituisci img_dataset con quello nuovo\nimg_dataset = AugmentedDataset(all_images, all_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:32:23.512575Z","iopub.execute_input":"2025-05-07T09:32:23.512782Z","iopub.status.idle":"2025-05-07T09:32:36.398782Z","shell.execute_reply.started":"2025-05-07T09:32:23.512765Z","shell.execute_reply":"2025-05-07T09:32:36.398087Z"}},"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4364/4364 [00:01<00:00, 2354.59it/s]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import Subset\n\n# Estrai le etichette in modo sicuro\nlabels = img_dataset.labels  # NON usare img_dataset[i][1]\n\n# Crea una lista di tutti gli indici\nall_indices = list(range(len(img_dataset)))\n\n# Split stratificato: Train (70%) e Temp (30%)\ntrain_idx, temp_idx = train_test_split(\n    all_indices, test_size=0.3, stratify=labels, random_state=42\n)\n\n# Estrai le label corrispondenti agli indici temporanei per secondo split\ntemp_labels = [labels[i] for i in temp_idx]\n\n# Split stratificato: Validation (15%) e Test (15%) da temp\nval_idx, test_idx = train_test_split(\n    temp_idx, test_size=0.5, stratify=temp_labels, random_state=42\n)\n\n# Crea i subset PyTorch\ntrain_ts = Subset(img_dataset, train_idx)\nval_ts = Subset(img_dataset, val_idx)\ntest_ts = Subset(img_dataset, test_idx)\n\n# Visualizzazione\nprint(\"train dataset size:\", len(train_ts))\nprint(\"validation dataset size:\", len(val_ts))\nprint(\"test dataset size:\", len(test_ts))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:34:13.387741Z","iopub.execute_input":"2025-05-07T09:34:13.388346Z","iopub.status.idle":"2025-05-07T09:34:13.464445Z","shell.execute_reply.started":"2025-05-07T09:34:13.388320Z","shell.execute_reply":"2025-05-07T09:34:13.463553Z"}},"outputs":[{"name":"stdout","text":"train dataset size: 15304\nvalidation dataset size: 3280\ntest dataset size: 3280\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# NEW MODEL: SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport torch\n\n# Reuse conversion function\ndef dataset_to_numpy(dataset):\n    X = []\n    y = []\n    for img, label in dataset:\n        X.append(img.view(-1).numpy())  # Flatten image\n        y.append(label)\n    return np.array(X), np.array(y)\n\n# Prepare data\nX_train, y_train = dataset_to_numpy(train_ts)\nX_val, y_val = dataset_to_numpy(val_ts)\nX_test, y_test = dataset_to_numpy(test_ts)\n\n# Normalize\nX_train = X_train / 255.0\nX_val = X_val / 255.0\nX_test = X_test / 255.0\n\n# Combine train and val for tuning\nX_combined = np.concatenate((X_train, X_val))\ny_combined = np.concatenate((y_train, y_val))\n\n\n# Define parameter grid\nC_values = [0.1, 1, 10]\nkernels = ['rbf', 'sigmoid', 'poly']\ngammas = ['scale', 'auto']\n\n# Track best model\nbest_acc = 0\nbest_params = None\nbest_model = None\n\n# Manual loop over all combinations\nfor C in C_values:\n    for kernel in kernels:\n        for gamma in gammas:\n            model = SVC(C=C, kernel=kernel, gamma=gamma)\n            model.fit(X_train, y_train)  # train ONLY on training set\n            val_preds = model.predict(X_val)\n            acc = accuracy_score(y_val, val_preds)\n            print(f\"Params: C={C}, kernel={kernel}, gamma={gamma} â†’ Val Accuracy: {acc:.4f}\")\n            \n            if acc > best_acc:\n                best_acc = acc\n                best_params = (C, kernel, gamma)\n                best_model = model\n\nprint(f\"\\nâœ… Best Params: C={best_params[0]}, kernel={best_params[1]}, gamma={best_params[2]}\")\nprint(f\"ğŸ“ˆ Best Validation Accuracy: {best_acc:.2%}\")\n\n# Evaluate best model on test set\ntest_preds = best_model.predict(X_test)\ntest_acc = accuracy_score(y_test, test_preds)\nprint(f\"\\nğŸ¯ Final Test Accuracy: {test_acc * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:35:17.411992Z","iopub.execute_input":"2025-05-07T09:35:17.412794Z","iopub.status.idle":"2025-05-07T11:37:18.855027Z","shell.execute_reply.started":"2025-05-07T09:35:17.412768Z","shell.execute_reply":"2025-05-07T11:37:18.853928Z"}},"outputs":[{"name":"stdout","text":"Params: C=0.1, kernel=rbf, gamma=scale â†’ Val Accuracy: 0.9210\nParams: C=0.1, kernel=rbf, gamma=auto â†’ Val Accuracy: 0.6006\nParams: C=0.1, kernel=sigmoid, gamma=scale â†’ Val Accuracy: 0.6006\nParams: C=0.1, kernel=sigmoid, gamma=auto â†’ Val Accuracy: 0.6006\nParams: C=0.1, kernel=poly, gamma=scale â†’ Val Accuracy: 0.9235\nParams: C=0.1, kernel=poly, gamma=auto â†’ Val Accuracy: 0.6006\nParams: C=1, kernel=rbf, gamma=scale â†’ Val Accuracy: 0.9491\nParams: C=1, kernel=rbf, gamma=auto â†’ Val Accuracy: 0.6006\nParams: C=1, kernel=sigmoid, gamma=scale â†’ Val Accuracy: 0.5927\nParams: C=1, kernel=sigmoid, gamma=auto â†’ Val Accuracy: 0.6006\nParams: C=1, kernel=poly, gamma=scale â†’ Val Accuracy: 0.9341\nParams: C=1, kernel=poly, gamma=auto â†’ Val Accuracy: 0.6006\nParams: C=10, kernel=rbf, gamma=scale â†’ Val Accuracy: 0.9561\nParams: C=10, kernel=rbf, gamma=auto â†’ Val Accuracy: 0.6006\nParams: C=10, kernel=sigmoid, gamma=scale â†’ Val Accuracy: 0.5201\nParams: C=10, kernel=sigmoid, gamma=auto â†’ Val Accuracy: 0.6006\nParams: C=10, kernel=poly, gamma=scale â†’ Val Accuracy: 0.9299\nParams: C=10, kernel=poly, gamma=auto â†’ Val Accuracy: 0.6006\n\nâœ… Best Params: C=10, kernel=rbf, gamma=scale\nğŸ“ˆ Best Validation Accuracy: 95.61%\n\nğŸ¯ Final Test Accuracy: 95.30%\n","output_type":"stream"}],"execution_count":15}]}