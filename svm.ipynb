{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:32:11.529001Z",
     "iopub.status.busy": "2025-05-07T09:32:11.528689Z",
     "iopub.status.idle": "2025-05-07T09:32:16.824322Z",
     "shell.execute_reply": "2025-05-07T09:32:16.823464Z",
     "shell.execute_reply.started": "2025-05-07T09:32:11.528921Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub[hf-datasets,pandas-datasets] in /usr/local/lib/python3.11/dist-packages (0.3.11)\n",
      "Collecting kagglehub[hf-datasets,pandas-datasets]\n",
      "  Downloading kagglehub-0.3.12-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub[hf-datasets,pandas-datasets]) (24.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub[hf-datasets,pandas-datasets]) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub[hf-datasets,pandas-datasets]) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub[hf-datasets,pandas-datasets]) (4.67.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from kagglehub[hf-datasets,pandas-datasets]) (3.5.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from kagglehub[hf-datasets,pandas-datasets]) (2.2.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (0.70.16)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->kagglehub[hf-datasets,pandas-datasets])\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (0.30.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[hf-datasets,pandas-datasets]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[hf-datasets,pandas-datasets]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[hf-datasets,pandas-datasets]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[hf-datasets,pandas-datasets]) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->kagglehub[hf-datasets,pandas-datasets]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->kagglehub[hf-datasets,pandas-datasets]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->kagglehub[hf-datasets,pandas-datasets]) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->kagglehub[hf-datasets,pandas-datasets]) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->kagglehub[hf-datasets,pandas-datasets]) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->kagglehub[hf-datasets,pandas-datasets]) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->kagglehub[hf-datasets,pandas-datasets]) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->kagglehub[hf-datasets,pandas-datasets]) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->kagglehub[hf-datasets,pandas-datasets]) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->kagglehub[hf-datasets,pandas-datasets]) (1.19.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets->kagglehub[hf-datasets,pandas-datasets]) (4.13.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->kagglehub[hf-datasets,pandas-datasets]) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (2024.2.0)\n",
      "Downloading kagglehub-0.3.12-py3-none-any.whl (67 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, kagglehub\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: kagglehub\n",
      "    Found existing installation: kagglehub 0.3.11\n",
      "    Uninstalling kagglehub-0.3.11:\n",
      "      Successfully uninstalled kagglehub-0.3.11\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2024.12.0 kagglehub-0.3.12\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade kagglehub[pandas-datasets,hf-datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:32:16.825963Z",
     "iopub.status.busy": "2025-05-07T09:32:16.825664Z",
     "iopub.status.idle": "2025-05-07T09:32:19.535697Z",
     "shell.execute_reply": "2025-05-07T09:32:19.534902Z",
     "shell.execute_reply.started": "2025-05-07T09:32:16.825931Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "git is already the newest version (1:2.34.1-1ubuntu1.12).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 122 not upgraded.\n",
      "fatal: destination path 'aml_1_aerial_imagery' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install git -y\n",
    "\n",
    "!git clone https://github.com/miriam-16/aml_1_aerial_imagery.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:32:19.537149Z",
     "iopub.status.busy": "2025-05-07T09:32:19.536810Z",
     "iopub.status.idle": "2025-05-07T09:32:22.880827Z",
     "shell.execute_reply": "2025-05-07T09:32:22.880167Z",
     "shell.execute_reply.started": "2025-05-07T09:32:19.537091Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageDraw\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "from torchvision import utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:32:22.883468Z",
     "iopub.status.busy": "2025-05-07T09:32:22.882939Z",
     "iopub.status.idle": "2025-05-07T09:32:22.911770Z",
     "shell.execute_reply": "2025-05-07T09:32:22.910816Z",
     "shell.execute_reply.started": "2025-05-07T09:32:22.883443Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id  has_cactus\n",
      "0  0004be2cfeaba1c0361d39e2b000257b.jpg           1\n",
      "1  000c8a36845c0208e833c79c1bffedd1.jpg           1\n",
      "2  000d1e9a533f62e55c289303b072733d.jpg           1\n",
      "3  0011485b40695e9138e92d0b3fb55128.jpg           1\n",
      "4  0014d7a11e90b62848904c1418fc8cf2.jpg           1\n"
     ]
    }
   ],
   "source": [
    "path = \"/kaggle/working/aml_1_aerial_imagery/dataset\"\n",
    "import pandas as pd\n",
    "\n",
    "labels_df = pd.read_csv(path+'/train.csv')  # Adjust filename\n",
    "print(labels_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:32:22.912978Z",
     "iopub.status.busy": "2025-05-07T09:32:22.912653Z",
     "iopub.status.idle": "2025-05-07T09:32:22.926255Z",
     "shell.execute_reply": "2025-05-07T09:32:22.925436Z",
     "shell.execute_reply.started": "2025-05-07T09:32:22.912950Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17500, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>has_cactus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, has_cactus]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(labels_df.shape)\n",
    "labels_df[labels_df.duplicated(keep=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:32:22.927281Z",
     "iopub.status.busy": "2025-05-07T09:32:22.927024Z",
     "iopub.status.idle": "2025-05-07T09:32:22.937016Z",
     "shell.execute_reply": "2025-05-07T09:32:22.936305Z",
     "shell.execute_reply.started": "2025-05-07T09:32:22.927254Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "has_cactus\n",
       "1    13136\n",
       "0     4364\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df['has_cactus'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:32:22.938033Z",
     "iopub.status.busy": "2025-05-07T09:32:22.937796Z",
     "iopub.status.idle": "2025-05-07T09:32:22.955879Z",
     "shell.execute_reply": "2025-05-07T09:32:22.955164Z",
     "shell.execute_reply.started": "2025-05-07T09:32:22.938013Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class pytorch_data(Dataset):\n",
    "    \n",
    "    def __init__(self, data_dir, transform, data_type=\"train\"):\n",
    "        # Get Image File Names\n",
    "        cdm_data = os.path.join(data_dir, data_type)\n",
    "        file_names = os.listdir(cdm_data)\n",
    "\n",
    "        all_image_paths = [os.path.join(cdm_data, f) for f in file_names if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "        print(f\"Found {len(all_image_paths)} images in directory.\")\n",
    "        print(f\"Sample filenames: {all_image_paths[:1]}\")\n",
    "\n",
    "        # Get Labels\n",
    "        labels_data = os.path.join(\"/kaggle/working/aml_1_aerial_imagery/dataset/\", \"train.csv\")\n",
    "        labels_df = pd.read_csv(labels_data)\n",
    "\n",
    "        # Normalize index: remove extensions if present\n",
    "        labels_df['id'] = labels_df['id'].apply(lambda x: os.path.splitext(str(x))[0])\n",
    "        labels_df.set_index(\"id\", inplace=True)\n",
    "\n",
    "        print(f\"Labels dataframe length: {len(labels_df)}\")\n",
    "\n",
    "        # Extract only valid images (that have a label)\n",
    "        valid_filenames = []\n",
    "        labels = []\n",
    "\n",
    "        for f in all_image_paths:\n",
    "            filename = os.path.basename(f)  # get only file name\n",
    "            image_id = os.path.splitext(filename)[0]  # remove extension\n",
    "\n",
    "            if image_id in labels_df.index:\n",
    "                valid_filenames.append(f)\n",
    "                labels.append(labels_df.loc[image_id].values[0])\n",
    "            else:\n",
    "                print(f\"Warning: image '{filename}' has no matching label in train.csv\")\n",
    "\n",
    "        self.full_filenames = valid_filenames\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f\"Valid image-label pairs: {len(self.full_filenames)}\")\n",
    "        print(f\"First few labels: {self.labels[:5]}\")\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.full_filenames)\n",
    "      \n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.full_filenames):\n",
    "            raise IndexError(f\"Index {idx} out of bounds for dataset of length {len(self.full_filenames)}\")\n",
    "\n",
    "        image = Image.open(self.full_filenames[idx])\n",
    "        image = self.transform(image)\n",
    "        return image, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:32:22.956986Z",
     "iopub.status.busy": "2025-05-07T09:32:22.956695Z",
     "iopub.status.idle": "2025-05-07T09:32:22.974688Z",
     "shell.execute_reply": "2025-05-07T09:32:22.973940Z",
     "shell.execute_reply.started": "2025-05-07T09:32:22.956959Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# define transformation that converts a PIL image into PyTorch tensors\n",
    "import torchvision.transforms as transforms\n",
    "data_transformer = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Resize((32,32))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:32:22.975692Z",
     "iopub.status.busy": "2025-05-07T09:32:22.975469Z",
     "iopub.status.idle": "2025-05-07T09:32:23.481590Z",
     "shell.execute_reply": "2025-05-07T09:32:23.480734Z",
     "shell.execute_reply.started": "2025-05-07T09:32:22.975674Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17500 images in directory.\n",
      "Sample filenames: ['/kaggle/working/aml_1_aerial_imagery/dataset/train/train/060a884c1005baed62bd6c4a72220da2.jpg']\n",
      "Labels dataframe length: 17500\n",
      "Valid image-label pairs: 17500\n",
      "First few labels: [0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Define an object of the custom dataset for the train folder.\n",
    "data_dir = path+'/train/'\n",
    "img_dataset = pytorch_data(data_dir, data_transformer, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:32:23.483653Z",
     "iopub.status.busy": "2025-05-07T09:32:23.483440Z",
     "iopub.status.idle": "2025-05-07T09:32:23.487749Z",
     "shell.execute_reply": "2025-05-07T09:32:23.486730Z",
     "shell.execute_reply.started": "2025-05-07T09:32:23.483636Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# define transformation that converts a PIL image into PyTorch tensors\n",
    "data_transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((32, 32))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:32:23.488625Z",
     "iopub.status.busy": "2025-05-07T09:32:23.488397Z",
     "iopub.status.idle": "2025-05-07T09:32:23.511845Z",
     "shell.execute_reply": "2025-05-07T09:32:23.510886Z",
     "shell.execute_reply.started": "2025-05-07T09:32:23.488608Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32]) tensor(0.1294) tensor(0.9412)\n"
     ]
    }
   ],
   "source": [
    "# Test a sample\n",
    "img, label = img_dataset[10]\n",
    "print(img.shape, torch.min(img), torch.max(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:32:23.512782Z",
     "iopub.status.busy": "2025-05-07T09:32:23.512575Z",
     "iopub.status.idle": "2025-05-07T09:32:36.398782Z",
     "shell.execute_reply": "2025-05-07T09:32:36.398087Z",
     "shell.execute_reply.started": "2025-05-07T09:32:23.512765Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4364/4364 [00:01<00:00, 2354.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# Aumentiamo le immagini della classe 0\n",
    "from torchvision.transforms import RandomRotation, ToTensor, Resize\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Trasformazione per augmentare\n",
    "augment_transform = transforms.Compose([\n",
    "    RandomRotation(degrees=10),\n",
    "    Resize((32, 32)),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "# Trova solo immagini con etichetta 0\n",
    "images_class0 = [i for i in range(len(img_dataset)) if img_dataset.labels[i] == 0]\n",
    "\n",
    "# Duplichiamo queste immagini con trasformazione\n",
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "\n",
    "for idx in tqdm(images_class0):\n",
    "    img_path = img_dataset.full_filenames[idx]\n",
    "    img = Image.open(img_path)\n",
    "    augmented_img = augment_transform(img)\n",
    "    augmented_images.append(augmented_img)\n",
    "    augmented_labels.append(0)\n",
    "\n",
    "# Stack immagini originali\n",
    "original_images = [img_dataset[i][0] for i in range(len(img_dataset))]\n",
    "original_labels = [img_dataset[i][1] for i in range(len(img_dataset))]\n",
    "\n",
    "# Combina immagini originali + augmentate\n",
    "all_images = torch.stack(original_images + augmented_images)\n",
    "all_labels = torch.tensor(original_labels + augmented_labels)\n",
    "\n",
    "# Nuovo Dataset custom con dati augmentati\n",
    "class AugmentedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "# Sostituisci img_dataset con quello nuovo\n",
    "img_dataset = AugmentedDataset(all_images, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:34:13.388346Z",
     "iopub.status.busy": "2025-05-07T09:34:13.387741Z",
     "iopub.status.idle": "2025-05-07T09:34:13.464445Z",
     "shell.execute_reply": "2025-05-07T09:34:13.463553Z",
     "shell.execute_reply.started": "2025-05-07T09:34:13.388320Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset size: 15304\n",
      "validation dataset size: 3280\n",
      "test dataset size: 3280\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Estrai le etichette in modo sicuro\n",
    "labels = img_dataset.labels  # NON usare img_dataset[i][1]\n",
    "\n",
    "# Crea una lista di tutti gli indici\n",
    "all_indices = list(range(len(img_dataset)))\n",
    "\n",
    "# Split stratificato: Train (70%) e Temp (30%)\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    all_indices, test_size=0.3, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Estrai le label corrispondenti agli indici temporanei per secondo split\n",
    "temp_labels = [labels[i] for i in temp_idx]\n",
    "\n",
    "# Split stratificato: Validation (15%) e Test (15%) da temp\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx, test_size=0.5, stratify=temp_labels, random_state=42\n",
    ")\n",
    "\n",
    "# Crea i subset PyTorch\n",
    "train_ts = Subset(img_dataset, train_idx)\n",
    "val_ts = Subset(img_dataset, val_idx)\n",
    "test_ts = Subset(img_dataset, test_idx)\n",
    "\n",
    "# Visualizzazione\n",
    "print(\"train dataset size:\", len(train_ts))\n",
    "print(\"validation dataset size:\", len(val_ts))\n",
    "print(\"test dataset size:\", len(test_ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW MODEL: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:35:17.412794Z",
     "iopub.status.busy": "2025-05-07T09:35:17.411992Z",
     "iopub.status.idle": "2025-05-07T11:37:18.855027Z",
     "shell.execute_reply": "2025-05-07T11:37:18.853928Z",
     "shell.execute_reply.started": "2025-05-07T09:35:17.412768Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: C=0.1, kernel=rbf, gamma=scale → Val Accuracy: 0.9210\n",
      "Params: C=0.1, kernel=rbf, gamma=auto → Val Accuracy: 0.6006\n",
      "Params: C=0.1, kernel=sigmoid, gamma=scale → Val Accuracy: 0.6006\n",
      "Params: C=0.1, kernel=sigmoid, gamma=auto → Val Accuracy: 0.6006\n",
      "Params: C=0.1, kernel=poly, gamma=scale → Val Accuracy: 0.9235\n",
      "Params: C=0.1, kernel=poly, gamma=auto → Val Accuracy: 0.6006\n",
      "Params: C=1, kernel=rbf, gamma=scale → Val Accuracy: 0.9491\n",
      "Params: C=1, kernel=rbf, gamma=auto → Val Accuracy: 0.6006\n",
      "Params: C=1, kernel=sigmoid, gamma=scale → Val Accuracy: 0.5927\n",
      "Params: C=1, kernel=sigmoid, gamma=auto → Val Accuracy: 0.6006\n",
      "Params: C=1, kernel=poly, gamma=scale → Val Accuracy: 0.9341\n",
      "Params: C=1, kernel=poly, gamma=auto → Val Accuracy: 0.6006\n",
      "Params: C=10, kernel=rbf, gamma=scale → Val Accuracy: 0.9561\n",
      "Params: C=10, kernel=rbf, gamma=auto → Val Accuracy: 0.6006\n",
      "Params: C=10, kernel=sigmoid, gamma=scale → Val Accuracy: 0.5201\n",
      "Params: C=10, kernel=sigmoid, gamma=auto → Val Accuracy: 0.6006\n",
      "Params: C=10, kernel=poly, gamma=scale → Val Accuracy: 0.9299\n",
      "Params: C=10, kernel=poly, gamma=auto → Val Accuracy: 0.6006\n",
      "\n",
      "✅ Best Params: C=10, kernel=rbf, gamma=scale\n",
      "📈 Best Validation Accuracy: 95.61%\n",
      "\n",
      "🎯 Final Test Accuracy: 95.30%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Reuse conversion function\n",
    "def dataset_to_numpy(dataset):\n",
    "    X = []\n",
    "    y = []\n",
    "    for img, label in dataset:\n",
    "        X.append(img.view(-1).numpy())  # Flatten image\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Prepare data\n",
    "X_train, y_train = dataset_to_numpy(train_ts)\n",
    "X_val, y_val = dataset_to_numpy(val_ts)\n",
    "X_test, y_test = dataset_to_numpy(test_ts)\n",
    "\n",
    "# Normalize\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Combine train and val for tuning\n",
    "X_combined = np.concatenate((X_train, X_val))\n",
    "y_combined = np.concatenate((y_train, y_val))\n",
    "\n",
    "\n",
    "# Define parameter grid\n",
    "C_values = [0.1, 1, 10]\n",
    "kernels = ['rbf', 'sigmoid', 'poly']\n",
    "gammas = ['scale', 'auto']\n",
    "\n",
    "# Track best model\n",
    "best_acc = 0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "# Manual loop over all combinations\n",
    "for C in C_values:\n",
    "    for kernel in kernels:\n",
    "        for gamma in gammas:\n",
    "            model = SVC(C=C, kernel=kernel, gamma=gamma)\n",
    "            model.fit(X_train, y_train)  # train ONLY on training set\n",
    "            val_preds = model.predict(X_val)\n",
    "            acc = accuracy_score(y_val, val_preds)\n",
    "            print(f\"Params: C={C}, kernel={kernel}, gamma={gamma} → Val Accuracy: {acc:.4f}\")\n",
    "            \n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_params = (C, kernel, gamma)\n",
    "                best_model = model\n",
    "\n",
    "print(f\"\\n✅ Best Params: C={best_params[0]}, kernel={best_params[1]}, gamma={best_params[2]}\")\n",
    "print(f\"📈 Best Validation Accuracy: {best_acc:.2%}\")\n",
    "\n",
    "# Evaluate best model on test set\n",
    "test_preds = best_model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, test_preds)\n",
    "print(f\"\\n🎯 Final Test Accuracy: {test_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T11:58:26.422038Z",
     "iopub.status.busy": "2025-05-07T11:58:26.421551Z",
     "iopub.status.idle": "2025-05-07T12:07:44.119896Z",
     "shell.execute_reply": "2025-05-07T12:07:44.118895Z",
     "shell.execute_reply.started": "2025-05-07T11:58:26.422007Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Test Accuracy: 0.9582317073170732\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9536    0.9412    0.9473      1309\n",
      "           1     0.9613    0.9696    0.9654      1971\n",
      "\n",
      "    accuracy                         0.9582      3280\n",
      "   macro avg     0.9574    0.9554    0.9564      3280\n",
      "weighted avg     0.9582    0.9582    0.9582      3280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Ritrain SVM con i parametri migliori su train + val\n",
    "C_best, kernel_best, gamma_best = best_params\n",
    "final_model = SVC(C=C_best, kernel=kernel_best, gamma=gamma_best)\n",
    "final_model.fit(X_combined, y_combined)  # Usa train + val\n",
    "\n",
    "# Predizioni sul test set\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "# Valutazione\n",
    "print(\"🎯 Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
