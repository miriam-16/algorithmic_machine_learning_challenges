{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11696839,"sourceType":"datasetVersion","datasetId":7341610}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nimport copy\nimport os\nimport torch\nfrom PIL import Image\nfrom PIL import Image, ImageDraw\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nfrom torch.utils.data import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch.nn as nn\nfrom torchvision import utils\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2025-05-06T17:33:09.052749Z","iopub.execute_input":"2025-05-06T17:33:09.053019Z","iopub.status.idle":"2025-05-06T17:33:20.203588Z","shell.execute_reply.started":"2025-05-06T17:33:09.052998Z","shell.execute_reply":"2025-05-06T17:33:20.202409Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"path = \"/kaggle/input/aml-challenge1\"\nimport pandas as pd\n\nlabels_df = pd.read_csv(path+'/train.csv')  # Adjust filename\nprint(labels_df.head())","metadata":{"execution":{"iopub.status.busy":"2025-05-06T17:33:30.420506Z","iopub.execute_input":"2025-05-06T17:33:30.421447Z","iopub.status.idle":"2025-05-06T17:33:30.481317Z","shell.execute_reply.started":"2025-05-06T17:33:30.421419Z","shell.execute_reply":"2025-05-06T17:33:30.480351Z"},"trusted":true},"outputs":[{"name":"stdout","text":"                                     id  has_cactus\n0  0004be2cfeaba1c0361d39e2b000257b.jpg           1\n1  000c8a36845c0208e833c79c1bffedd1.jpg           1\n2  000d1e9a533f62e55c289303b072733d.jpg           1\n3  0011485b40695e9138e92d0b3fb55128.jpg           1\n4  0014d7a11e90b62848904c1418fc8cf2.jpg           1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"print(labels_df.shape)\nlabels_df[labels_df.duplicated(keep=False)]","metadata":{"execution":{"iopub.status.busy":"2025-05-06T17:33:33.695774Z","iopub.execute_input":"2025-05-06T17:33:33.696697Z","iopub.status.idle":"2025-05-06T17:33:33.735806Z","shell.execute_reply.started":"2025-05-06T17:33:33.696660Z","shell.execute_reply":"2025-05-06T17:33:33.734976Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(17500, 2)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [id, has_cactus]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>has_cactus</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"labels_df['has_cactus'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2025-05-06T17:33:36.846948Z","iopub.execute_input":"2025-05-06T17:33:36.847935Z","iopub.status.idle":"2025-05-06T17:33:36.858504Z","shell.execute_reply.started":"2025-05-06T17:33:36.847897Z","shell.execute_reply":"2025-05-06T17:33:36.857308Z"},"trusted":true},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"has_cactus\n1    13136\n0     4364\nName: count, dtype: int64"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import torch\ntorch.manual_seed(0)\n\nfrom torch.utils.data import Dataset\nimport os\nimport pandas as pd\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\nclass pytorch_data(Dataset):\n    \n    def __init__(self, data_dir, transform, data_type=\"train\"):\n        # Get Image File Names\n        cdm_data = os.path.join(data_dir, data_type)\n        file_names = os.listdir(cdm_data)\n\n        all_image_paths = [os.path.join(cdm_data, f) for f in file_names if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n\n        print(f\"Found {len(all_image_paths)} images in directory.\")\n        print(f\"Sample filenames: {all_image_paths[:1]}\")\n\n        # Get Labels\n        labels_data = os.path.join(\"/kaggle/input/aml-challenge1/\", \"train.csv\")\n        labels_df = pd.read_csv(labels_data)\n\n        # Normalize index: remove extensions if present\n        labels_df['id'] = labels_df['id'].apply(lambda x: os.path.splitext(str(x))[0])\n        labels_df.set_index(\"id\", inplace=True)\n\n        print(f\"Labels dataframe length: {len(labels_df)}\")\n\n        # Extract only valid images (that have a label)\n        valid_filenames = []\n        labels = []\n\n        for f in all_image_paths:\n            filename = os.path.basename(f)  # get only file name\n            image_id = os.path.splitext(filename)[0]  # remove extension\n\n            if image_id in labels_df.index:\n                valid_filenames.append(f)\n                labels.append(labels_df.loc[image_id].values[0])\n            else:\n                print(f\"Warning: image '{filename}' has no matching label in train.csv\")\n\n        self.full_filenames = valid_filenames\n        self.labels = labels\n        self.transform = transform\n\n        print(f\"Valid image-label pairs: {len(self.full_filenames)}\")\n        print(f\"First few labels: {self.labels[:5]}\")\n      \n    def __len__(self):\n        return len(self.full_filenames)\n      \n    def __getitem__(self, idx):\n        if idx >= len(self.full_filenames):\n            raise IndexError(f\"Index {idx} out of bounds for dataset of length {len(self.full_filenames)}\")\n\n        image = Image.open(self.full_filenames[idx])\n        image = self.transform(image)\n        return image, self.labels[idx]","metadata":{"execution":{"iopub.status.busy":"2025-05-06T17:33:40.728351Z","iopub.execute_input":"2025-05-06T17:33:40.728729Z","iopub.status.idle":"2025-05-06T17:33:40.746784Z","shell.execute_reply.started":"2025-05-06T17:33:40.728705Z","shell.execute_reply":"2025-05-06T17:33:40.745988Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# define transformation that converts a PIL image into PyTorch tensors\nimport torchvision.transforms as transforms\ndata_transformer = transforms.Compose([transforms.ToTensor(),\n                                       transforms.Resize((32,32))])","metadata":{"execution":{"iopub.status.busy":"2025-05-06T17:33:44.709079Z","iopub.execute_input":"2025-05-06T17:33:44.709402Z","iopub.status.idle":"2025-05-06T17:33:44.714497Z","shell.execute_reply.started":"2025-05-06T17:33:44.709381Z","shell.execute_reply":"2025-05-06T17:33:44.713552Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Define an object of the custom dataset for the train folder.\ndata_dir = path+'/train/'\nimg_dataset = pytorch_data(data_dir, data_transformer, \"train\")","metadata":{"execution":{"iopub.status.busy":"2025-05-06T17:33:47.479916Z","iopub.execute_input":"2025-05-06T17:33:47.480228Z","iopub.status.idle":"2025-05-06T17:33:48.333742Z","shell.execute_reply.started":"2025-05-06T17:33:47.480206Z","shell.execute_reply":"2025-05-06T17:33:48.332779Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Found 17500 images in directory.\nSample filenames: ['/kaggle/input/aml-challenge1/train/train/5d3a7d32516a92cc0dc8c52af515eaa4.jpg']\nLabels dataframe length: 17500\nValid image-label pairs: 17500\nFirst few labels: [1, 0, 1, 1, 1]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# define transformation that converts a PIL image into PyTorch tensors\ndata_transformer = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((32, 32))\n])","metadata":{"execution":{"iopub.status.busy":"2025-05-06T17:33:51.148547Z","iopub.execute_input":"2025-05-06T17:33:51.148860Z","iopub.status.idle":"2025-05-06T17:33:51.154002Z","shell.execute_reply.started":"2025-05-06T17:33:51.148837Z","shell.execute_reply":"2025-05-06T17:33:51.153031Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Test a sample\nimg, label = img_dataset[10]\nprint(img.shape, torch.min(img), torch.max(img))","metadata":{"execution":{"iopub.status.busy":"2025-05-06T17:34:00.041809Z","iopub.execute_input":"2025-05-06T17:34:00.042857Z","iopub.status.idle":"2025-05-06T17:34:00.197870Z","shell.execute_reply.started":"2025-05-06T17:34:00.042827Z","shell.execute_reply":"2025-05-06T17:34:00.196660Z"},"trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([3, 32, 32]) tensor(0.2667) tensor(0.8627)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"**DATA AUGMENTATION CHE VA A RADDOPPIARE IL NUMERO DI SAMPLE DELLA CLASSE 0 AVENDO ALLA FINE CHE I SAMPLE DELLA CLASSE 0 SONO I 2/3 DEI SAMPLE DELLA CLASSE 1**","metadata":{}},{"cell_type":"code","source":"# Aumentiamo le immagini della classe 0\nfrom torchvision.transforms import RandomRotation, ToTensor, Resize\nfrom tqdm import tqdm\n\n# Trasformazione per augmentare\naugment_transform = transforms.Compose([\n    RandomRotation(degrees=10),\n    Resize((32, 32)),\n    ToTensor()\n])\n\n# Trova solo immagini con etichetta 0\nimages_class0 = [i for i in range(len(img_dataset)) if img_dataset.labels[i] == 0]\n\n# Duplichiamo queste immagini con trasformazione\naugmented_images = []\naugmented_labels = []\n\nfor idx in tqdm(images_class0):\n    img_path = img_dataset.full_filenames[idx]\n    img = Image.open(img_path)\n    augmented_img = augment_transform(img)\n    augmented_images.append(augmented_img)\n    augmented_labels.append(0)\n\n# Stack immagini originali\noriginal_images = [img_dataset[i][0] for i in range(len(img_dataset))]\noriginal_labels = [img_dataset[i][1] for i in range(len(img_dataset))]\n\n# Combina immagini originali + augmentate\nall_images = torch.stack(original_images + augmented_images)\nall_labels = torch.tensor(original_labels + augmented_labels)\n\n# Nuovo Dataset custom con dati augmentati\nclass AugmentedDataset(torch.utils.data.Dataset):\n    def __init__(self, images, labels):\n        self.images = images\n        self.labels = labels\n    def __len__(self):\n        return len(self.images)\n    def __getitem__(self, idx):\n        return self.images[idx], self.labels[idx]\n\n# Sostituisci img_dataset con quello nuovo\nimg_dataset = AugmentedDataset(all_images, all_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:34:03.558783Z","iopub.execute_input":"2025-05-06T17:34:03.559220Z","iopub.status.idle":"2025-05-06T17:36:14.219781Z","shell.execute_reply.started":"2025-05-06T17:34:03.559195Z","shell.execute_reply":"2025-05-06T17:36:14.218729Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 4364/4364 [00:27<00:00, 161.51it/s]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"***DIVISIONE NEI TRE SET TRAIN, VALIDATION, TEST DOPO AVER AUMENTATO I DATI DELLA CLASSE 0***","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import Subset\n\n# Estrai le etichette in modo sicuro\nlabels = img_dataset.labels  # NON usare img_dataset[i][1]\n\n# Crea una lista di tutti gli indici\nall_indices = list(range(len(img_dataset)))\n\n# Split stratificato: Train (70%) e Temp (30%)\ntrain_idx, temp_idx = train_test_split(\n    all_indices, test_size=0.3, stratify=labels, random_state=42\n)\n\n# Estrai le label corrispondenti agli indici temporanei per secondo split\ntemp_labels = [labels[i] for i in temp_idx]\n\n# Split stratificato: Validation (15%) e Test (15%) da temp\nval_idx, test_idx = train_test_split(\n    temp_idx, test_size=0.5, stratify=temp_labels, random_state=42\n)\n\n# Crea i subset PyTorch\ntrain_ts = Subset(img_dataset, train_idx)\nval_ts = Subset(img_dataset, val_idx)\ntest_ts = Subset(img_dataset, test_idx)\n\n# Visualizzazione\nprint(\"train dataset size:\", len(train_ts))\nprint(\"validation dataset size:\", len(val_ts))\nprint(\"test dataset size:\", len(test_ts))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:36:19.019308Z","iopub.execute_input":"2025-05-06T17:36:19.019701Z","iopub.status.idle":"2025-05-06T17:36:20.069241Z","shell.execute_reply.started":"2025-05-06T17:36:19.019673Z","shell.execute_reply":"2025-05-06T17:36:20.068258Z"}},"outputs":[{"name":"stdout","text":"train dataset size: 15304\nvalidation dataset size: 3280\ntest dataset size: 3280\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"**DATA LOADER FOR SPLITTING IMAGES**","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Training DataLoader\ntrain_dl = DataLoader(train_ts,\n                      batch_size=64, \n                      shuffle=True)\n\n# Validation DataLoader\nval_dl = DataLoader(val_ts,\n                    batch_size=64,\n                    shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:36:27.095820Z","iopub.execute_input":"2025-05-06T17:36:27.097012Z","iopub.status.idle":"2025-05-06T17:36:27.102350Z","shell.execute_reply.started":"2025-05-06T17:36:27.096983Z","shell.execute_reply":"2025-05-06T17:36:27.101078Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from torchvision import models\nimport torch.nn as nn\n\n# Carica ResNet18 senza pesi pre-addestrati\nmodel = models.resnet18(weights=None)\n\n# Modifica l'output per classificazione binaria\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Sequential(\n    nn.Linear(num_ftrs, 1),\n    nn.Sigmoid()\n)\n\n# Manda su GPU se disponibile\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:39:56.487424Z","iopub.execute_input":"2025-05-06T17:39:56.488561Z","iopub.status.idle":"2025-05-06T17:39:56.732164Z","shell.execute_reply.started":"2025-05-06T17:39:56.488524Z","shell.execute_reply":"2025-05-06T17:39:56.731275Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from torchvision import models\nimport torch.optim as optim\nimport torch.nn as nn\n\n# Griglia di iperparametri da esplorare\nlearning_rates = [1e-3, 1e-4]\noptimizers = ['adam', 'sgd']\n\nresults = []\n\nfor lr in learning_rates:\n    for opt_name in optimizers:\n        # Inizializza un nuovo modello ResNet18\n        model = models.resnet18(weights=None)\n        num_ftrs = model.fc.in_features\n        model.fc = nn.Sequential(nn.Linear(num_ftrs, 1), nn.Sigmoid())\n        model = model.to(device)\n\n        # Crea ottimizzatore\n        if opt_name == 'adam':\n            optimizer = optim.Adam(model.fc.parameters(), lr=lr)\n        else:\n            optimizer = optim.SGD(model.fc.parameters(), lr=lr, momentum=0.9)\n\n        # Loss e scheduler\n        criterion = nn.BCELoss()\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2)\n\n        # Allenamento su 3 epoche per tuning veloce\n        for epoch in range(15):\n            train_loss, train_acc = train_epoch(model, train_dl, optimizer, criterion)\n            val_loss, val_acc = evaluate(model, val_dl, criterion)\n            scheduler.step(val_loss)\n\n        results.append({\n            'lr': lr,\n            'optimizer': opt_name,\n            'val_loss': val_loss,\n            'val_acc': val_acc\n        })\n\n        print(f\"lr={lr}, opt={opt_name}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T18:04:28.320442Z","iopub.execute_input":"2025-05-06T18:04:28.320844Z"}},"outputs":[{"name":"stdout","text":"lr=0.001, opt=adam, val_loss=0.2906, val_acc=0.8881\n","output_type":"stream"}],"execution_count":null}]}