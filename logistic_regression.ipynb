{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade kagglehub[pandas-datasets,hf-datasets]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:08:27.422415Z","iopub.execute_input":"2025-05-06T16:08:27.422637Z","iopub.status.idle":"2025-05-06T16:08:33.793843Z","shell.execute_reply.started":"2025-05-06T16:08:27.422615Z","shell.execute_reply":"2025-05-06T16:08:33.792725Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: kagglehub[hf-datasets,pandas-datasets] in /usr/local/lib/python3.11/dist-packages (0.3.11)\nCollecting kagglehub[hf-datasets,pandas-datasets]\n  Downloading kagglehub-0.3.12-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub[hf-datasets,pandas-datasets]) (24.2)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub[hf-datasets,pandas-datasets]) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub[hf-datasets,pandas-datasets]) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub[hf-datasets,pandas-datasets]) (4.67.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from kagglehub[hf-datasets,pandas-datasets]) (3.5.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from kagglehub[hf-datasets,pandas-datasets]) (2.2.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->kagglehub[hf-datasets,pandas-datasets])\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (3.11.16)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets->kagglehub[hf-datasets,pandas-datasets]) (0.30.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[hf-datasets,pandas-datasets]) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[hf-datasets,pandas-datasets]) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[hf-datasets,pandas-datasets]) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[hf-datasets,pandas-datasets]) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->kagglehub[hf-datasets,pandas-datasets]) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->kagglehub[hf-datasets,pandas-datasets]) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->kagglehub[hf-datasets,pandas-datasets]) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->kagglehub[hf-datasets,pandas-datasets]) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->kagglehub[hf-datasets,pandas-datasets]) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->kagglehub[hf-datasets,pandas-datasets]) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->kagglehub[hf-datasets,pandas-datasets]) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->kagglehub[hf-datasets,pandas-datasets]) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->kagglehub[hf-datasets,pandas-datasets]) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->kagglehub[hf-datasets,pandas-datasets]) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets->kagglehub[hf-datasets,pandas-datasets]) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->kagglehub[hf-datasets,pandas-datasets]) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets->kagglehub[hf-datasets,pandas-datasets]) (2024.2.0)\nDownloading kagglehub-0.3.12-py3-none-any.whl (67 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, kagglehub\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: kagglehub\n    Found existing installation: kagglehub 0.3.11\n    Uninstalling kagglehub-0.3.11:\n      Successfully uninstalled kagglehub-0.3.11\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2024.12.0 kagglehub-0.3.12\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!apt-get install git -y\n\n!git clone https://github.com/miriam-16/aml_1_aerial_imagery.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:08:51.012131Z","iopub.execute_input":"2025-05-06T16:08:51.012455Z","iopub.status.idle":"2025-05-06T16:08:56.936974Z","shell.execute_reply.started":"2025-05-06T16:08:51.012426Z","shell.execute_reply":"2025-05-06T16:08:56.935823Z"}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ngit is already the newest version (1:2.34.1-1ubuntu1.12).\n0 upgraded, 0 newly installed, 0 to remove and 122 not upgraded.\nCloning into 'aml_1_aerial_imagery'...\nremote: Enumerating objects: 21535, done.\u001b[K\nremote: Counting objects: 100% (21535/21535), done.\u001b[K\nremote: Compressing objects: 100% (3105/3105), done.\u001b[K\nremote: Total 21535 (delta 18426), reused 21531 (delta 18422), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (21535/21535), 12.63 MiB | 43.10 MiB/s, done.\nResolving deltas: 100% (18426/18426), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nimport copy\nimport os\nimport torch\nfrom PIL import Image\nfrom PIL import Image, ImageDraw\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nfrom torch.utils.data import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch.nn as nn\nfrom torchvision import utils\n%matplotlib inline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:08:58.779928Z","iopub.execute_input":"2025-05-06T16:08:58.780362Z","iopub.status.idle":"2025-05-06T16:09:07.923197Z","shell.execute_reply.started":"2025-05-06T16:08:58.780331Z","shell.execute_reply":"2025-05-06T16:09:07.922316Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"path = \"/kaggle/working/aml_1_aerial_imagery/dataset\"\nimport pandas as pd\n\nlabels_df = pd.read_csv(path+'/train.csv')  # Adjust filename\nprint(labels_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:20:41.389085Z","iopub.execute_input":"2025-05-06T16:20:41.389922Z","iopub.status.idle":"2025-05-06T16:20:41.416360Z","shell.execute_reply.started":"2025-05-06T16:20:41.389885Z","shell.execute_reply":"2025-05-06T16:20:41.415358Z"}},"outputs":[{"name":"stdout","text":"                                     id  has_cactus\n0  0004be2cfeaba1c0361d39e2b000257b.jpg           1\n1  000c8a36845c0208e833c79c1bffedd1.jpg           1\n2  000d1e9a533f62e55c289303b072733d.jpg           1\n3  0011485b40695e9138e92d0b3fb55128.jpg           1\n4  0014d7a11e90b62848904c1418fc8cf2.jpg           1\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"print(labels_df.shape)\nlabels_df[labels_df.duplicated(keep=False)]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:20:43.741448Z","iopub.execute_input":"2025-05-06T16:20:43.741827Z","iopub.status.idle":"2025-05-06T16:20:43.755904Z","shell.execute_reply.started":"2025-05-06T16:20:43.741794Z","shell.execute_reply":"2025-05-06T16:20:43.755128Z"}},"outputs":[{"name":"stdout","text":"(17500, 2)\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [id, has_cactus]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>has_cactus</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"labels_df['has_cactus'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:09:12.463972Z","iopub.execute_input":"2025-05-06T16:09:12.464302Z","iopub.status.idle":"2025-05-06T16:09:12.474062Z","shell.execute_reply.started":"2025-05-06T16:09:12.464277Z","shell.execute_reply":"2025-05-06T16:09:12.473321Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"has_cactus\n1    13136\n0     4364\nName: count, dtype: int64"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import torch\ntorch.manual_seed(0)\n\nfrom torch.utils.data import Dataset\nimport os\nimport pandas as pd\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\nclass pytorch_data(Dataset):\n    \n    def __init__(self, data_dir, transform, data_type=\"train\"):\n        # Get Image File Names\n        cdm_data = os.path.join(data_dir, data_type)\n        file_names = os.listdir(cdm_data)\n\n        all_image_paths = [os.path.join(cdm_data, f) for f in file_names if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n\n        print(f\"Found {len(all_image_paths)} images in directory.\")\n        print(f\"Sample filenames: {all_image_paths[:1]}\")\n\n        # Get Labels\n        labels_data = os.path.join(\"/kaggle/working/aml_1_aerial_imagery/dataset/\", \"train.csv\")\n        labels_df = pd.read_csv(labels_data)\n\n        # Normalize index: remove extensions if present\n        labels_df['id'] = labels_df['id'].apply(lambda x: os.path.splitext(str(x))[0])\n        labels_df.set_index(\"id\", inplace=True)\n\n        print(f\"Labels dataframe length: {len(labels_df)}\")\n\n        # Extract only valid images (that have a label)\n        valid_filenames = []\n        labels = []\n\n        for f in all_image_paths:\n            filename = os.path.basename(f)  # get only file name\n            image_id = os.path.splitext(filename)[0]  # remove extension\n\n            if image_id in labels_df.index:\n                valid_filenames.append(f)\n                labels.append(labels_df.loc[image_id].values[0])\n            else:\n                print(f\"Warning: image '{filename}' has no matching label in train.csv\")\n\n        self.full_filenames = valid_filenames\n        self.labels = labels\n        self.transform = transform\n\n        print(f\"Valid image-label pairs: {len(self.full_filenames)}\")\n        print(f\"First few labels: {self.labels[:5]}\")\n      \n    def __len__(self):\n        return len(self.full_filenames)\n      \n    def __getitem__(self, idx):\n        if idx >= len(self.full_filenames):\n            raise IndexError(f\"Index {idx} out of bounds for dataset of length {len(self.full_filenames)}\")\n\n        image = Image.open(self.full_filenames[idx])\n        image = self.transform(image)\n        return image, self.labels[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:20:46.532983Z","iopub.execute_input":"2025-05-06T16:20:46.533352Z","iopub.status.idle":"2025-05-06T16:20:46.547894Z","shell.execute_reply.started":"2025-05-06T16:20:46.533329Z","shell.execute_reply":"2025-05-06T16:20:46.546805Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# define transformation that converts a PIL image into PyTorch tensors\nimport torchvision.transforms as transforms\ndata_transformer = transforms.Compose([transforms.ToTensor(),\n                                       transforms.Resize((32,32))])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:20:49.455624Z","iopub.execute_input":"2025-05-06T16:20:49.455934Z","iopub.status.idle":"2025-05-06T16:20:49.461883Z","shell.execute_reply.started":"2025-05-06T16:20:49.455915Z","shell.execute_reply":"2025-05-06T16:20:49.460653Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# Define an object of the custom dataset for the train folder.\ndata_dir = path+'/train/'\nimg_dataset = pytorch_data(data_dir, data_transformer, \"train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:20:51.168643Z","iopub.execute_input":"2025-05-06T16:20:51.170167Z","iopub.status.idle":"2025-05-06T16:20:51.731320Z","shell.execute_reply.started":"2025-05-06T16:20:51.170121Z","shell.execute_reply":"2025-05-06T16:20:51.730526Z"}},"outputs":[{"name":"stdout","text":"Found 17500 images in directory.\nSample filenames: ['/kaggle/working/aml_1_aerial_imagery/dataset/train/train/d61b9446a08b2e46d03ee74ba2642239.jpg']\nLabels dataframe length: 17500\nValid image-label pairs: 17500\nFirst few labels: [1, 1, 1, 1, 1]\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"# define transformation that converts a PIL image into PyTorch tensors\ndata_transformer = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((32, 32))\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:20:53.850275Z","iopub.execute_input":"2025-05-06T16:20:53.850673Z","iopub.status.idle":"2025-05-06T16:20:53.856396Z","shell.execute_reply.started":"2025-05-06T16:20:53.850648Z","shell.execute_reply":"2025-05-06T16:20:53.855080Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# Test a sample\nimg, label = img_dataset[10]\nprint(img.shape, torch.min(img), torch.max(img))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:20:57.920625Z","iopub.execute_input":"2025-05-06T16:20:57.920969Z","iopub.status.idle":"2025-05-06T16:20:57.930338Z","shell.execute_reply.started":"2025-05-06T16:20:57.920946Z","shell.execute_reply":"2025-05-06T16:20:57.929158Z"}},"outputs":[{"name":"stdout","text":"torch.Size([3, 32, 32]) tensor(0.0471) tensor(0.7176)\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"# Aumentiamo le immagini della classe 0\nfrom torchvision.transforms import RandomRotation, ToTensor, Resize\nfrom tqdm import tqdm\n\n# Trasformazione per augmentare\naugment_transform = transforms.Compose([\n    RandomRotation(degrees=10),\n    Resize((32, 32)),\n    ToTensor()\n])\n\n# Trova solo immagini con etichetta 0\nimages_class0 = [i for i in range(len(img_dataset)) if img_dataset.labels[i] == 0]\n\n# Duplichiamo queste immagini con trasformazione\naugmented_images = []\naugmented_labels = []\n\nfor idx in tqdm(images_class0):\n    img_path = img_dataset.full_filenames[idx]\n    img = Image.open(img_path)\n    augmented_img = augment_transform(img)\n    augmented_images.append(augmented_img)\n    augmented_labels.append(0)\n\n# Stack immagini originali\noriginal_images = [img_dataset[i][0] for i in range(len(img_dataset))]\noriginal_labels = [img_dataset[i][1] for i in range(len(img_dataset))]\n\n# Combina immagini originali + augmentate\nall_images = torch.stack(original_images + augmented_images)\nall_labels = torch.tensor(original_labels + augmented_labels)\n\n# Nuovo Dataset custom con dati augmentati\nclass AugmentedDataset(torch.utils.data.Dataset):\n    def __init__(self, images, labels):\n        self.images = images\n        self.labels = labels\n    def __len__(self):\n        return len(self.images)\n    def __getitem__(self, idx):\n        return self.images[idx], self.labels[idx]\n\n# Sostituisci img_dataset con quello nuovo\nimg_dataset = AugmentedDataset(all_images, all_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:21:01.038378Z","iopub.execute_input":"2025-05-06T16:21:01.038961Z","iopub.status.idle":"2025-05-06T16:21:14.536721Z","shell.execute_reply.started":"2025-05-06T16:21:01.038938Z","shell.execute_reply":"2025-05-06T16:21:14.535811Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 4364/4364 [00:01<00:00, 2246.98it/s]\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import Subset\n\n# Estrai le etichette in modo sicuro\nlabels = img_dataset.labels  # NON usare img_dataset[i][1]\n\n# Crea una lista di tutti gli indici\nall_indices = list(range(len(img_dataset)))\n\n# Split stratificato: Train (70%) e Temp (30%)\ntrain_idx, temp_idx = train_test_split(\n    all_indices, test_size=0.3, stratify=labels, random_state=42\n)\n\n# Estrai le label corrispondenti agli indici temporanei per secondo split\ntemp_labels = [labels[i] for i in temp_idx]\n\n# Split stratificato: Validation (15%) e Test (15%) da temp\nval_idx, test_idx = train_test_split(\n    temp_idx, test_size=0.5, stratify=temp_labels, random_state=42\n)\n\n# Crea i subset PyTorch\ntrain_ts = Subset(img_dataset, train_idx)\nval_ts = Subset(img_dataset, val_idx)\ntest_ts = Subset(img_dataset, test_idx)\n\n# Visualizzazione\nprint(\"train dataset size:\", len(train_ts))\nprint(\"validation dataset size:\", len(val_ts))\nprint(\"test dataset size:\", len(test_ts))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:21:35.816999Z","iopub.execute_input":"2025-05-06T16:21:35.817380Z","iopub.status.idle":"2025-05-06T16:21:35.904396Z","shell.execute_reply.started":"2025-05-06T16:21:35.817358Z","shell.execute_reply":"2025-05-06T16:21:35.903520Z"}},"outputs":[{"name":"stdout","text":"train dataset size: 15304\nvalidation dataset size: 3280\ntest dataset size: 3280\n","output_type":"stream"}],"execution_count":51},{"cell_type":"markdown","source":"# NEW MODEL: LOGISTIC REGRESSION","metadata":{}},{"cell_type":"code","source":"def initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w, b\n\ndef sigmoid(z):\n    y_head = 1/(1+np.exp(-z))\n    return y_head\n\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/x_train.shape[1]\n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost,gradients\n\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    for i in range(number_of_iterarion):\n        \n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        \n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 100 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n\ndef predict(w,b,x_test):\n    \n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n\n    dimension =  x_train.shape[0]\n    w,b = initialize_weights_and_bias(dimension)\n\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n    \n    print(\"Test Accuracy: {} %\".format(round(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100,2)))\n    print(\"Train Accuracy: {} %\".format(round(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100,2)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:35:33.801947Z","iopub.execute_input":"2025-05-06T16:35:33.802496Z","iopub.status.idle":"2025-05-06T16:35:33.818662Z","shell.execute_reply.started":"2025-05-06T16:35:33.802445Z","shell.execute_reply":"2025-05-06T16:35:33.817556Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"import numpy as np\n\ndef subset_to_numpy(subset):\n    images = []\n    labels = []\n    for img, label in subset:\n        images.append(img.numpy().flatten())  # flatten 3D tensor to 1D vector\n        labels.append(label)\n    X = np.array(images).T  # shape: (features, samples)\n    Y = np.array(labels).reshape(1, -1)  # shape: (1, samples)\n    return X, Y\n\n# Convert datasets to NumPy\nx_train, y_train = subset_to_numpy(train_ts)\nx_test, y_test = subset_to_numpy(test_ts)\n\n# Call your logistic regression function\nlogistic_regression(x_train, y_train, x_test, y_test, learning_rate=0.01, num_iterations=3500)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:35:37.621338Z","iopub.execute_input":"2025-05-06T16:35:37.622221Z","iopub.status.idle":"2025-05-06T16:41:47.421544Z","shell.execute_reply.started":"2025-05-06T16:35:37.622190Z","shell.execute_reply":"2025-05-06T16:41:47.420603Z"}},"outputs":[{"name":"stdout","text":"Cost after iteration 0: 5.954573\nCost after iteration 100: 0.541984\nCost after iteration 200: 0.481981\nCost after iteration 300: 0.447249\nCost after iteration 400: 0.424166\nCost after iteration 500: 0.407546\nCost after iteration 600: 0.394933\nCost after iteration 700: 0.384991\nCost after iteration 800: 0.376920\nCost after iteration 900: 0.370212\nCost after iteration 1000: 0.364525\nCost after iteration 1100: 0.359623\nCost after iteration 1200: 0.355338\nCost after iteration 1300: 0.351546\nCost after iteration 1400: 0.348152\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Test Accuracy: 83.48 %\nTrain Accuracy: 84.21 %\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"def grid_search_logistic_regression(x_train, y_train, x_val, y_val, learning_rates, iteration_counts):\n    best_accuracy = 0\n    best_params = {}\n    \n    for lr in learning_rates:\n        for num_iter in iteration_counts:\n            print(f\"\\nTraining with learning_rate={lr}, num_iterations={num_iter}\")\n            \n            dimension = x_train.shape[0]\n            w, b = initialize_weights_and_bias(dimension)\n            parameters, gradients, _ = update(w, b, x_train, y_train, lr, num_iter)\n            \n            y_pred_val = predict(parameters[\"weight\"], parameters[\"bias\"], x_val)\n            accuracy = 100 - np.mean(np.abs(y_pred_val - y_val)) * 100\n            print(f\"Validation Accuracy: {accuracy:.2f}%\")\n            \n            if accuracy > best_accuracy:\n                best_accuracy = accuracy\n                best_params = {\n                    \"learning_rate\": lr,\n                    \"num_iterations\": num_iter,\n                    \"weight\": parameters[\"weight\"],\n                    \"bias\": parameters[\"bias\"]\n                }\n    \n    print(f\"\\nBest parameters: {best_params}\")\n    print(f\"Best validation accuracy: {best_accuracy:.2f}%\")\n    return best_params\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:51:08.806032Z","iopub.execute_input":"2025-05-06T16:51:08.806348Z","iopub.status.idle":"2025-05-06T16:51:08.814569Z","shell.execute_reply.started":"2025-05-06T16:51:08.806329Z","shell.execute_reply":"2025-05-06T16:51:08.813386Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"# Convert val set\nx_val, y_val = subset_to_numpy(val_ts)\n\n# Define search space\nlearning_rates = [0.1, 1, 10]\niteration_counts = [500, 1000, 1500]\n\n# Run grid search\nbest = grid_search_logistic_regression(x_train, y_train, x_val, y_val, learning_rates, iteration_counts)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport torch\n\n# Reuse conversion function\ndef dataset_to_numpy(dataset):\n    X = []\n    y = []\n    for img, label in dataset:\n        X.append(img.view(-1).numpy())  # Flatten image\n        y.append(label)\n    return np.array(X), np.array(y)\n\n# Prepare data\nX_train, y_train = dataset_to_numpy(train_ts)\nX_val, y_val = dataset_to_numpy(val_ts)\nX_test, y_test = dataset_to_numpy(test_ts)\n\n# Normalize\nX_train = X_train / 255.0\nX_val = X_val / 255.0\nX_test = X_test / 255.0\n\n# Combine train and val for tuning\nX_combined = np.concatenate((X_train, X_val))\ny_combined = np.concatenate((y_train, y_val))\n\n# Define parameter grid\nparam_grid = {\n    'C': [0.1, 1, 10],               # Inverse regularization strength\n    'solver': ['lbfgs'],                   # Recommended solver for binary problems\n    'penalty': ['l2'],                     # Regularization type\n    'max_iter': [1500, 3000]                # Ensure convergence\n}\n\n# Initialize and perform grid search\ngrid_search = GridSearchCV(LogisticRegression(), param_grid, cv=3, verbose=1, scoring='accuracy')\ngrid_search.fit(X_combined, y_combined)\n\n# Print best parameters\nprint(\"Best Parameters:\", grid_search.best_params_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:09:32.536671Z","iopub.execute_input":"2025-05-06T17:09:32.537056Z","iopub.status.idle":"2025-05-06T17:10:07.110892Z","shell.execute_reply.started":"2025-05-06T17:09:32.537031Z","shell.execute_reply":"2025-05-06T17:10:07.107871Z"}},"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 6 candidates, totalling 18 fits\nBest Parameters: {'C': 10, 'max_iter': 1500, 'penalty': 'l2', 'solver': 'lbfgs'}\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Combine training and validation sets\nX_train_final = np.concatenate((X_train, X_val))\ny_train_final = np.concatenate((y_train, y_val))\n\n# Retrain with best parameters\nfinal_model = LogisticRegression(\n    C=10,\n    penalty='l2',\n    solver='lbfgs',\n    max_iter=1500\n)\nfinal_model.fit(X_train_final, y_train_final)\n\n# Predict on test set\ny_pred_test = final_model.predict(X_test)\n\n# Evaluate\ntest_accuracy = accuracy_score(y_test, y_pred_test)\nprint(f\"✅ Final Test Accuracy: {test_accuracy * 100:.2f}%\\n\")\n\n# Optional: Detailed classification report\nprint(\"📊 Classification Report:\")\nprint(classification_report(y_test, y_pred_test))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:10:55.664986Z","iopub.execute_input":"2025-05-06T17:10:55.665306Z","iopub.status.idle":"2025-05-06T17:11:01.344049Z","shell.execute_reply.started":"2025-05-06T17:10:55.665285Z","shell.execute_reply":"2025-05-06T17:11:01.342992Z"}},"outputs":[{"name":"stdout","text":"✅ Final Test Accuracy: 81.01%\n\n📊 Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      0.55      0.70      1309\n           1       0.77      0.99      0.86      1971\n\n    accuracy                           0.81      3280\n   macro avg       0.86      0.77      0.78      3280\nweighted avg       0.84      0.81      0.80      3280\n\n","output_type":"stream"}],"execution_count":62}]}