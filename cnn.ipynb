{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11696839,"sourceType":"datasetVersion","datasetId":7341610}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nimport copy\nimport os\nimport torch\nfrom PIL import Image\nfrom PIL import Image, ImageDraw\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nfrom torch.utils.data import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch.nn as nn\nfrom torchvision import utils\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2025-05-06T14:35:33.979102Z","iopub.execute_input":"2025-05-06T14:35:33.979508Z","iopub.status.idle":"2025-05-06T14:35:33.990040Z","shell.execute_reply.started":"2025-05-06T14:35:33.979481Z","shell.execute_reply":"2025-05-06T14:35:33.988923Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"path = \"/kaggle/input/aml-challenge1\"\nimport pandas as pd\n\nlabels_df = pd.read_csv(path+'/train.csv')  # Adjust filename\nprint(labels_df.head())","metadata":{"execution":{"iopub.status.busy":"2025-05-06T14:35:33.994053Z","iopub.execute_input":"2025-05-06T14:35:33.994438Z","iopub.status.idle":"2025-05-06T14:35:34.045925Z","shell.execute_reply.started":"2025-05-06T14:35:33.994411Z","shell.execute_reply":"2025-05-06T14:35:34.044975Z"},"trusted":true},"outputs":[{"name":"stdout","text":"                                     id  has_cactus\n0  0004be2cfeaba1c0361d39e2b000257b.jpg           1\n1  000c8a36845c0208e833c79c1bffedd1.jpg           1\n2  000d1e9a533f62e55c289303b072733d.jpg           1\n3  0011485b40695e9138e92d0b3fb55128.jpg           1\n4  0014d7a11e90b62848904c1418fc8cf2.jpg           1\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(labels_df.shape)\nlabels_df[labels_df.duplicated(keep=False)]","metadata":{"execution":{"iopub.status.busy":"2025-05-06T14:35:34.047869Z","iopub.execute_input":"2025-05-06T14:35:34.048248Z","iopub.status.idle":"2025-05-06T14:35:34.063187Z","shell.execute_reply.started":"2025-05-06T14:35:34.048224Z","shell.execute_reply":"2025-05-06T14:35:34.062243Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(17500, 2)\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [id, has_cactus]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>has_cactus</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"labels_df['has_cactus'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2025-05-06T14:35:34.064342Z","iopub.execute_input":"2025-05-06T14:35:34.064758Z","iopub.status.idle":"2025-05-06T14:35:34.076882Z","shell.execute_reply.started":"2025-05-06T14:35:34.064715Z","shell.execute_reply":"2025-05-06T14:35:34.075783Z"},"trusted":true},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"has_cactus\n1    13136\n0     4364\nName: count, dtype: int64"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"import torch\ntorch.manual_seed(0)\n\nfrom torch.utils.data import Dataset\nimport os\nimport pandas as pd\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\nclass pytorch_data(Dataset):\n    \n    def __init__(self, data_dir, transform, data_type=\"train\"):\n        # Get Image File Names\n        cdm_data = os.path.join(data_dir, data_type)\n        file_names = os.listdir(cdm_data)\n\n        all_image_paths = [os.path.join(cdm_data, f) for f in file_names if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n\n        print(f\"Found {len(all_image_paths)} images in directory.\")\n        print(f\"Sample filenames: {all_image_paths[:1]}\")\n\n        # Get Labels\n        labels_data = os.path.join(\"/kaggle/input/aml-challenge1/\", \"train.csv\")\n        labels_df = pd.read_csv(labels_data)\n\n        # Normalize index: remove extensions if present\n        labels_df['id'] = labels_df['id'].apply(lambda x: os.path.splitext(str(x))[0])\n        labels_df.set_index(\"id\", inplace=True)\n\n        print(f\"Labels dataframe length: {len(labels_df)}\")\n\n        # Extract only valid images (that have a label)\n        valid_filenames = []\n        labels = []\n\n        for f in all_image_paths:\n            filename = os.path.basename(f)  # get only file name\n            image_id = os.path.splitext(filename)[0]  # remove extension\n\n            if image_id in labels_df.index:\n                valid_filenames.append(f)\n                labels.append(labels_df.loc[image_id].values[0])\n            else:\n                print(f\"Warning: image '{filename}' has no matching label in train.csv\")\n\n        self.full_filenames = valid_filenames\n        self.labels = labels\n        self.transform = transform\n\n        print(f\"Valid image-label pairs: {len(self.full_filenames)}\")\n        print(f\"First few labels: {self.labels[:5]}\")\n      \n    def __len__(self):\n        return len(self.full_filenames)\n      \n    def __getitem__(self, idx):\n        if idx >= len(self.full_filenames):\n            raise IndexError(f\"Index {idx} out of bounds for dataset of length {len(self.full_filenames)}\")\n\n        image = Image.open(self.full_filenames[idx])\n        image = self.transform(image)\n        return image, self.labels[idx]","metadata":{"execution":{"iopub.status.busy":"2025-05-06T14:35:34.079831Z","iopub.execute_input":"2025-05-06T14:35:34.080235Z","iopub.status.idle":"2025-05-06T14:35:34.100169Z","shell.execute_reply.started":"2025-05-06T14:35:34.080207Z","shell.execute_reply":"2025-05-06T14:35:34.099165Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# define transformation that converts a PIL image into PyTorch tensors\nimport torchvision.transforms as transforms\ndata_transformer = transforms.Compose([transforms.ToTensor(),\n                                       transforms.Resize((32,32))])","metadata":{"execution":{"iopub.status.busy":"2025-05-06T14:35:34.101267Z","iopub.execute_input":"2025-05-06T14:35:34.101632Z","iopub.status.idle":"2025-05-06T14:35:34.121132Z","shell.execute_reply.started":"2025-05-06T14:35:34.101587Z","shell.execute_reply":"2025-05-06T14:35:34.119861Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Define an object of the custom dataset for the train folder.\ndata_dir = path+'/train/'\nimg_dataset = pytorch_data(data_dir, data_transformer, \"train\")","metadata":{"execution":{"iopub.status.busy":"2025-05-06T14:35:34.122288Z","iopub.execute_input":"2025-05-06T14:35:34.122706Z","iopub.status.idle":"2025-05-06T14:35:34.663493Z","shell.execute_reply.started":"2025-05-06T14:35:34.122661Z","shell.execute_reply":"2025-05-06T14:35:34.662656Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Found 17500 images in directory.\nSample filenames: ['/kaggle/input/aml-challenge1/train/train/5d3a7d32516a92cc0dc8c52af515eaa4.jpg']\nLabels dataframe length: 17500\nValid image-label pairs: 17500\nFirst few labels: [1, 0, 1, 1, 1]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# define transformation that converts a PIL image into PyTorch tensors\ndata_transformer = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((32, 32))\n])","metadata":{"execution":{"iopub.status.busy":"2025-05-06T14:35:34.664780Z","iopub.execute_input":"2025-05-06T14:35:34.665183Z","iopub.status.idle":"2025-05-06T14:35:34.670130Z","shell.execute_reply.started":"2025-05-06T14:35:34.665149Z","shell.execute_reply":"2025-05-06T14:35:34.669232Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Test a sample\nimg, label = img_dataset[10]\nprint(img.shape, torch.min(img), torch.max(img))","metadata":{"execution":{"iopub.status.busy":"2025-05-06T14:35:34.671160Z","iopub.execute_input":"2025-05-06T14:35:34.671483Z","iopub.status.idle":"2025-05-06T14:35:34.699184Z","shell.execute_reply.started":"2025-05-06T14:35:34.671453Z","shell.execute_reply":"2025-05-06T14:35:34.697999Z"},"trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([3, 32, 32]) tensor(0.2667) tensor(0.8627)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"**DATA AUGMENTATION CHE VA A RADDOPPIARE IL NUMERO DI SAMPLE DELLA CLASSE 0 AVENDO ALLA FINE CHE I SAMPLE DELLA CLASSE 0 SONO I 2/3 DEI SAMPLE DELLA CLASSE 1**","metadata":{}},{"cell_type":"code","source":"# Aumentiamo le immagini della classe 0\nfrom torchvision.transforms import RandomRotation, ToTensor, Resize\nfrom tqdm import tqdm\n\n# Trasformazione per augmentare\naugment_transform = transforms.Compose([\n    RandomRotation(degrees=10),\n    Resize((32, 32)),\n    ToTensor()\n])\n\n# Trova solo immagini con etichetta 0\nimages_class0 = [i for i in range(len(img_dataset)) if img_dataset.labels[i] == 0]\n\n# Duplichiamo queste immagini con trasformazione\naugmented_images = []\naugmented_labels = []\n\nfor idx in tqdm(images_class0):\n    img_path = img_dataset.full_filenames[idx]\n    img = Image.open(img_path)\n    augmented_img = augment_transform(img)\n    augmented_images.append(augmented_img)\n    augmented_labels.append(0)\n\n# Stack immagini originali\noriginal_images = [img_dataset[i][0] for i in range(len(img_dataset))]\noriginal_labels = [img_dataset[i][1] for i in range(len(img_dataset))]\n\n# Combina immagini originali + augmentate\nall_images = torch.stack(original_images + augmented_images)\nall_labels = torch.tensor(original_labels + augmented_labels)\n\n# Nuovo Dataset custom con dati augmentati\nclass AugmentedDataset(torch.utils.data.Dataset):\n    def __init__(self, images, labels):\n        self.images = images\n        self.labels = labels\n    def __len__(self):\n        return len(self.images)\n    def __getitem__(self, idx):\n        return self.images[idx], self.labels[idx]\n\n# Sostituisci img_dataset con quello nuovo\nimg_dataset = AugmentedDataset(all_images, all_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:45:47.390514Z","iopub.execute_input":"2025-05-06T14:45:47.390852Z","iopub.status.idle":"2025-05-06T14:46:49.264681Z","shell.execute_reply.started":"2025-05-06T14:45:47.390826Z","shell.execute_reply":"2025-05-06T14:46:49.263678Z"}},"outputs":[{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4364/4364 [00:10<00:00, 399.11it/s]\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"***DIVISIONE NEI TRE SET TRAIN, VALIDATION, TEST DOPO AVER AUMENTATO I DATI DELLA CLASSE 0***","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import Subset\n\n# Estrai le etichette in modo sicuro\nlabels = img_dataset.labels  # NON usare img_dataset[i][1]\n\n# Crea una lista di tutti gli indici\nall_indices = list(range(len(img_dataset)))\n\n# Split stratificato: Train (70%) e Temp (30%)\ntrain_idx, temp_idx = train_test_split(\n    all_indices, test_size=0.3, stratify=labels, random_state=42\n)\n\n# Estrai le label corrispondenti agli indici temporanei per secondo split\ntemp_labels = [labels[i] for i in temp_idx]\n\n# Split stratificato: Validation (15%) e Test (15%) da temp\nval_idx, test_idx = train_test_split(\n    temp_idx, test_size=0.5, stratify=temp_labels, random_state=42\n)\n\n# Crea i subset PyTorch\ntrain_ts = Subset(img_dataset, train_idx)\nval_ts = Subset(img_dataset, val_idx)\ntest_ts = Subset(img_dataset, test_idx)\n\n# Visualizzazione\nprint(\"train dataset size:\", len(train_ts))\nprint(\"validation dataset size:\", len(val_ts))\nprint(\"test dataset size:\", len(test_ts))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:16:22.595389Z","iopub.execute_input":"2025-05-06T15:16:22.595681Z","iopub.status.idle":"2025-05-06T15:16:23.146797Z","shell.execute_reply.started":"2025-05-06T15:16:22.595661Z","shell.execute_reply":"2025-05-06T15:16:23.146037Z"}},"outputs":[{"name":"stdout","text":"train dataset size: 15304\nvalidation dataset size: 3280\ntest dataset size: 3280\n","output_type":"stream"}],"execution_count":45},{"cell_type":"markdown","source":"**DATA LOADER FOR SPLITTING IMAGES**","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Training DataLoader\ntrain_dl = DataLoader(train_ts,\n                      batch_size=64, \n                      shuffle=True)\n\n# Validation DataLoader\nval_dl = DataLoader(val_ts,\n                    batch_size=64,\n                    shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:37:15.569972Z","iopub.execute_input":"2025-05-06T15:37:15.570693Z","iopub.status.idle":"2025-05-06T15:37:15.576890Z","shell.execute_reply.started":"2025-05-06T15:37:15.570657Z","shell.execute_reply":"2025-05-06T15:37:15.575672Z"}},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":"**DEFINIZIONE DEL MODELLO CNN**","metadata":{}},{"cell_type":"code","source":"def findConv2dOutShape(hin,win,conv,pool=2):\n    # get conv arguments\n    kernel_size=conv.kernel_size\n    stride=conv.stride\n    padding=conv.padding\n    dilation=conv.dilation\n\n    hout=np.floor((hin+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0]+1)\n    wout=np.floor((win+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1]+1)\n\n    if pool:\n        hout/=pool\n        wout/=pool\n    return int(hout),int(wout)\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Neural Network\nclass Network(nn.Module):\n    \n    # Network Initialisation\n    def __init__(self, params):\n        \n        super(Network, self).__init__()\n    \n        Cin,Hin,Win=params[\"shape_in\"]\n        init_f=params[\"initial_filters\"] \n        num_fc1=params[\"num_fc1\"]  \n        num_classes=params[\"num_classes\"] \n        self.dropout_rate=params[\"dropout_rate\"] \n        \n        # Convolution Layers\n        self.conv1 = nn.Conv2d(Cin, init_f, kernel_size=3, padding=1)\n        h,w=findConv2dOutShape(Hin,Win,self.conv1)\n        self.conv2 = nn.Conv2d(init_f, 2*init_f, kernel_size=3,padding=1)\n        h,w=findConv2dOutShape(h,w,self.conv2)\n        self.conv3 = nn.Conv2d(2*init_f, 4*init_f, kernel_size=3, padding=1)\n        h,w=findConv2dOutShape(h,w,self.conv3)\n        \n        # compute the flatten size\n        self.num_flatten=h*w*4*init_f   #4*8 -> 32\n        self.fc1 = nn.Linear(self.num_flatten, num_fc1)\n        self.fc2 = nn.Linear(num_fc1, num_classes)\n\n    \n\n    def forward(self,X):\n        # Convolution & Pool Layers\n        X = F.relu(self.conv1(X));\n        X = F.max_pool2d(X, 2, 2)\n        X = F.relu(self.conv2(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = F.relu(self.conv3(X))\n        X = F.max_pool2d(X, 2, 2)\n\n        b = X.shape[0]\n        X = X.view(b,-1) #torch appiattisce su tutti i canali, tranne il primo che corrisponde alla batch size\n        \n        X = F.relu(self.fc1(X))\n        X=F.dropout(X, self.dropout_rate)\n        X = self.fc2(X)\n        return F.log_softmax(X, dim=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:37:18.603580Z","iopub.execute_input":"2025-05-06T15:37:18.604502Z","iopub.status.idle":"2025-05-06T15:37:18.617237Z","shell.execute_reply.started":"2025-05-06T15:37:18.604469Z","shell.execute_reply":"2025-05-06T15:37:18.616202Z"}},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":"**PARAMETRI DEL MODELLO**","metadata":{}},{"cell_type":"code","source":"# Neural Network Predefined Parameters\nparams_model={\n        \"shape_in\": (3,32,32), \n        \"initial_filters\": 8,    \n        \"num_fc1\": 100,\n        \"dropout_rate\": 0.25,\n        \"num_classes\": 2}\n\n# Create instantiation of Network class\ncnn_model = Network(params_model)\n\n# define computation hardware approach (GPU/CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = cnn_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:37:23.015084Z","iopub.execute_input":"2025-05-06T15:37:23.015384Z","iopub.status.idle":"2025-05-06T15:37:23.030743Z","shell.execute_reply.started":"2025-05-06T15:37:23.015363Z","shell.execute_reply":"2025-05-06T15:37:23.029959Z"}},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":"**GRID SEARCH PER IL TUNING DEI PARAMETRI**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\n\n# üîß Hyperparameter grid\nlr_list = [0.001, 0.005, 0.01]\ndropout_list = [0.25, 0.4, 0.5]\nfilters_list = [8, 16, 32]\nfc1_list = [64, 100, 128]\n\n# üì¶ Funzione per addestramento + validazione\ndef train_and_evaluate(model, train_dl, val_dl, lr, device, epochs=5):\n    model = model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.NLLLoss()\n\n    model.train()\n    for epoch in range(epochs):\n        for X_batch, y_batch in train_dl:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            optimizer.zero_grad()\n            output = model(X_batch)\n            loss = criterion(output, y_batch)\n            loss.backward()\n            optimizer.step()\n\n    model.eval()\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for X_batch, y_batch in val_dl:\n            X_batch = X_batch.to(device)\n            output = model(X_batch)\n            preds = output.argmax(dim=1).cpu().numpy()\n            y_pred.extend(preds)\n            y_true.extend(y_batch.numpy())\n\n    acc = accuracy_score(y_true, y_pred)\n    return acc\n\n# üöÄ Grid Search\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbest_acc = 0\nbest_params = {}\n\nfor lr in lr_list:\n    for dropout in dropout_list:\n        for filters in filters_list:\n            for fc1 in fc1_list:\n                params_model = {\n                    \"shape_in\": (3, 32, 32),\n                    \"initial_filters\": filters,\n                    \"num_fc1\": fc1,\n                    \"dropout_rate\": dropout,\n                    \"num_classes\": 2\n                }\n\n                model = Network(params_model)\n                acc = train_and_evaluate(model, train_dl, val_dl, lr, device, epochs=5)\n\n                print(f\"LR: {lr}, Dropout: {dropout}, Filters: {filters}, FC1: {fc1} -> Val Accuracy: {acc:.4f}\")\n\n                if acc > best_acc:\n                    best_acc = acc\n                    best_params = {\n                        \"lr\": lr,\n                        \"dropout_rate\": dropout,\n                        \"initial_filters\": filters,\n                        \"num_fc1\": fc1\n                    }\n\nprint(f\"üèÜ Best Accuracy: {best_acc:.4f} with params {best_params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:48:48.007366Z","iopub.execute_input":"2025-05-06T15:48:48.008522Z","iopub.status.idle":"2025-05-06T16:45:49.961193Z","shell.execute_reply.started":"2025-05-06T15:48:48.008487Z","shell.execute_reply":"2025-05-06T16:45:49.959996Z"}},"outputs":[{"name":"stdout","text":"LR: 0.001, Dropout: 0.25, Filters: 8, FC1: 64 -> Val Accuracy: 0.9637\nLR: 0.001, Dropout: 0.25, Filters: 8, FC1: 100 -> Val Accuracy: 0.9387\nLR: 0.001, Dropout: 0.25, Filters: 8, FC1: 128 -> Val Accuracy: 0.9619\nLR: 0.001, Dropout: 0.25, Filters: 16, FC1: 64 -> Val Accuracy: 0.9695\nLR: 0.001, Dropout: 0.25, Filters: 16, FC1: 100 -> Val Accuracy: 0.9793\nLR: 0.001, Dropout: 0.25, Filters: 16, FC1: 128 -> Val Accuracy: 0.9689\nLR: 0.001, Dropout: 0.25, Filters: 32, FC1: 64 -> Val Accuracy: 0.9823\nLR: 0.001, Dropout: 0.25, Filters: 32, FC1: 100 -> Val Accuracy: 0.9811\nLR: 0.001, Dropout: 0.25, Filters: 32, FC1: 128 -> Val Accuracy: 0.9817\nLR: 0.001, Dropout: 0.4, Filters: 8, FC1: 64 -> Val Accuracy: 0.9610\nLR: 0.001, Dropout: 0.4, Filters: 8, FC1: 100 -> Val Accuracy: 0.9622\nLR: 0.001, Dropout: 0.4, Filters: 8, FC1: 128 -> Val Accuracy: 0.9649\nLR: 0.001, Dropout: 0.4, Filters: 16, FC1: 64 -> Val Accuracy: 0.9540\nLR: 0.001, Dropout: 0.4, Filters: 16, FC1: 100 -> Val Accuracy: 0.9726\nLR: 0.001, Dropout: 0.4, Filters: 16, FC1: 128 -> Val Accuracy: 0.9488\nLR: 0.001, Dropout: 0.4, Filters: 32, FC1: 64 -> Val Accuracy: 0.9601\nLR: 0.001, Dropout: 0.4, Filters: 32, FC1: 100 -> Val Accuracy: 0.9646\nLR: 0.001, Dropout: 0.4, Filters: 32, FC1: 128 -> Val Accuracy: 0.9683\nLR: 0.001, Dropout: 0.5, Filters: 8, FC1: 64 -> Val Accuracy: 0.9601\nLR: 0.001, Dropout: 0.5, Filters: 8, FC1: 100 -> Val Accuracy: 0.9369\nLR: 0.001, Dropout: 0.5, Filters: 8, FC1: 128 -> Val Accuracy: 0.9643\nLR: 0.001, Dropout: 0.5, Filters: 16, FC1: 64 -> Val Accuracy: 0.9637\nLR: 0.001, Dropout: 0.5, Filters: 16, FC1: 100 -> Val Accuracy: 0.9348\nLR: 0.001, Dropout: 0.5, Filters: 16, FC1: 128 -> Val Accuracy: 0.9790\nLR: 0.001, Dropout: 0.5, Filters: 32, FC1: 64 -> Val Accuracy: 0.9774\nLR: 0.001, Dropout: 0.5, Filters: 32, FC1: 100 -> Val Accuracy: 0.9787\nLR: 0.001, Dropout: 0.5, Filters: 32, FC1: 128 -> Val Accuracy: 0.9564\nLR: 0.005, Dropout: 0.25, Filters: 8, FC1: 64 -> Val Accuracy: 0.9601\nLR: 0.005, Dropout: 0.25, Filters: 8, FC1: 100 -> Val Accuracy: 0.9741\nLR: 0.005, Dropout: 0.25, Filters: 8, FC1: 128 -> Val Accuracy: 0.9604\nLR: 0.005, Dropout: 0.25, Filters: 16, FC1: 64 -> Val Accuracy: 0.9729\nLR: 0.005, Dropout: 0.25, Filters: 16, FC1: 100 -> Val Accuracy: 0.9689\nLR: 0.005, Dropout: 0.25, Filters: 16, FC1: 128 -> Val Accuracy: 0.9488\nLR: 0.005, Dropout: 0.25, Filters: 32, FC1: 64 -> Val Accuracy: 0.9671\nLR: 0.005, Dropout: 0.25, Filters: 32, FC1: 100 -> Val Accuracy: 0.9759\nLR: 0.005, Dropout: 0.25, Filters: 32, FC1: 128 -> Val Accuracy: 0.9732\nLR: 0.005, Dropout: 0.4, Filters: 8, FC1: 64 -> Val Accuracy: 0.9613\nLR: 0.005, Dropout: 0.4, Filters: 8, FC1: 100 -> Val Accuracy: 0.9677\nLR: 0.005, Dropout: 0.4, Filters: 8, FC1: 128 -> Val Accuracy: 0.9582\nLR: 0.005, Dropout: 0.4, Filters: 16, FC1: 64 -> Val Accuracy: 0.9701\nLR: 0.005, Dropout: 0.4, Filters: 16, FC1: 100 -> Val Accuracy: 0.9738\nLR: 0.005, Dropout: 0.4, Filters: 16, FC1: 128 -> Val Accuracy: 0.9780\nLR: 0.005, Dropout: 0.4, Filters: 32, FC1: 64 -> Val Accuracy: 0.9665\nLR: 0.005, Dropout: 0.4, Filters: 32, FC1: 100 -> Val Accuracy: 0.9759\nLR: 0.005, Dropout: 0.4, Filters: 32, FC1: 128 -> Val Accuracy: 0.9762\nLR: 0.005, Dropout: 0.5, Filters: 8, FC1: 64 -> Val Accuracy: 0.9735\nLR: 0.005, Dropout: 0.5, Filters: 8, FC1: 100 -> Val Accuracy: 0.9659\nLR: 0.005, Dropout: 0.5, Filters: 8, FC1: 128 -> Val Accuracy: 0.9595\nLR: 0.005, Dropout: 0.5, Filters: 16, FC1: 64 -> Val Accuracy: 0.9186\nLR: 0.005, Dropout: 0.5, Filters: 16, FC1: 100 -> Val Accuracy: 0.9168\nLR: 0.005, Dropout: 0.5, Filters: 16, FC1: 128 -> Val Accuracy: 0.9713\nLR: 0.005, Dropout: 0.5, Filters: 32, FC1: 64 -> Val Accuracy: 0.8759\nLR: 0.005, Dropout: 0.5, Filters: 32, FC1: 100 -> Val Accuracy: 0.9704\nLR: 0.005, Dropout: 0.5, Filters: 32, FC1: 128 -> Val Accuracy: 0.9689\nLR: 0.01, Dropout: 0.25, Filters: 8, FC1: 64 -> Val Accuracy: 0.6006\nLR: 0.01, Dropout: 0.25, Filters: 8, FC1: 100 -> Val Accuracy: 0.9369\nLR: 0.01, Dropout: 0.25, Filters: 8, FC1: 128 -> Val Accuracy: 0.9591\nLR: 0.01, Dropout: 0.25, Filters: 16, FC1: 64 -> Val Accuracy: 0.9326\nLR: 0.01, Dropout: 0.25, Filters: 16, FC1: 100 -> Val Accuracy: 0.9515\nLR: 0.01, Dropout: 0.25, Filters: 16, FC1: 128 -> Val Accuracy: 0.6006\nLR: 0.01, Dropout: 0.25, Filters: 32, FC1: 64 -> Val Accuracy: 0.9576\nLR: 0.01, Dropout: 0.25, Filters: 32, FC1: 100 -> Val Accuracy: 0.9625\nLR: 0.01, Dropout: 0.25, Filters: 32, FC1: 128 -> Val Accuracy: 0.9601\nLR: 0.01, Dropout: 0.4, Filters: 8, FC1: 64 -> Val Accuracy: 0.9430\nLR: 0.01, Dropout: 0.4, Filters: 8, FC1: 100 -> Val Accuracy: 0.7591\nLR: 0.01, Dropout: 0.4, Filters: 8, FC1: 128 -> Val Accuracy: 0.9503\nLR: 0.01, Dropout: 0.4, Filters: 16, FC1: 64 -> Val Accuracy: 0.9485\nLR: 0.01, Dropout: 0.4, Filters: 16, FC1: 100 -> Val Accuracy: 0.9622\nLR: 0.01, Dropout: 0.4, Filters: 16, FC1: 128 -> Val Accuracy: 0.6006\nLR: 0.01, Dropout: 0.4, Filters: 32, FC1: 64 -> Val Accuracy: 0.9476\nLR: 0.01, Dropout: 0.4, Filters: 32, FC1: 100 -> Val Accuracy: 0.9549\nLR: 0.01, Dropout: 0.4, Filters: 32, FC1: 128 -> Val Accuracy: 0.9610\nLR: 0.01, Dropout: 0.5, Filters: 8, FC1: 64 -> Val Accuracy: 0.9357\nLR: 0.01, Dropout: 0.5, Filters: 8, FC1: 100 -> Val Accuracy: 0.9631\nLR: 0.01, Dropout: 0.5, Filters: 8, FC1: 128 -> Val Accuracy: 0.6006\nLR: 0.01, Dropout: 0.5, Filters: 16, FC1: 64 -> Val Accuracy: 0.7591\nLR: 0.01, Dropout: 0.5, Filters: 16, FC1: 100 -> Val Accuracy: 0.9549\nLR: 0.01, Dropout: 0.5, Filters: 16, FC1: 128 -> Val Accuracy: 0.9622\nLR: 0.01, Dropout: 0.5, Filters: 32, FC1: 64 -> Val Accuracy: 0.9268\nLR: 0.01, Dropout: 0.5, Filters: 32, FC1: 100 -> Val Accuracy: 0.9448\nLR: 0.01, Dropout: 0.5, Filters: 32, FC1: 128 -> Val Accuracy: 0.9430\nüèÜ Best Accuracy: 0.9823 with params {'lr': 0.001, 'dropout_rate': 0.25, 'initial_filters': 32, 'num_fc1': 64}\n","output_type":"stream"}],"execution_count":57},{"cell_type":"markdown","source":"**USO I BEST IPERPARAMETRI SUL TEST**\n\nüèÜ Best Accuracy: 0.9823 with params {'lr': 0.001, 'dropout_rate': 0.25, 'initial_filters': 32, 'num_fc1': 64}","metadata":{}},{"cell_type":"code","source":"best_params = {\n    'lr': 0.001,\n    'dropout_rate': 0.25,\n    'initial_filters': 32,\n    'num_fc1': 64\n}\n\n# Costruzione del modello finale\nfinal_params = {\n    \"shape_in\": (3, 32, 32),\n    \"initial_filters\": best_params[\"initial_filters\"],\n    \"num_fc1\": best_params[\"num_fc1\"],\n    \"dropout_rate\": best_params[\"dropout_rate\"],\n    \"num_classes\": 2\n}\n\nfinal_model = Network(final_params).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:52:01.658895Z","iopub.execute_input":"2025-05-06T16:52:01.659279Z","iopub.status.idle":"2025-05-06T16:52:01.669310Z","shell.execute_reply.started":"2025-05-06T16:52:01.659254Z","shell.execute_reply":"2025-05-06T16:52:01.668390Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"from torch.utils.data import ConcatDataset, DataLoader\n\n# Combina train + validation set\nfull_train_set = ConcatDataset([train_ts, val_ts])\nfull_train_loader = DataLoader(full_train_set, batch_size=64, shuffle=True)\n\n# Ottimizzatore e loss\noptimizer = torch.optim.Adam(final_model.parameters(), lr=best_params[\"lr\"])\ncriterion = nn.NLLLoss()\n\n# Training\nfinal_model.train()\nfor epoch in range(10):\n    running_loss = 0.0\n    for X_batch, y_batch in full_train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n        output = final_model(X_batch)\n        loss = criterion(output, y_batch)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    print(f\"Epoch {epoch+1} - Loss: {running_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:52:05.042754Z","iopub.execute_input":"2025-05-06T16:52:05.043121Z","iopub.status.idle":"2025-05-06T16:55:08.665262Z","shell.execute_reply.started":"2025-05-06T16:52:05.043092Z","shell.execute_reply":"2025-05-06T16:55:08.664410Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 - Loss: 65.1843\nEpoch 2 - Loss: 25.6979\nEpoch 3 - Loss: 17.9817\nEpoch 4 - Loss: 16.0125\nEpoch 5 - Loss: 12.0859\nEpoch 6 - Loss: 10.3995\nEpoch 7 - Loss: 7.9387\nEpoch 8 - Loss: 7.8818\nEpoch 9 - Loss: 7.4810\nEpoch 10 - Loss: 5.5941\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"test_loader = DataLoader(test_ts, batch_size=64, shuffle=False)\n\n# Valutazione\nfinal_model.eval()\ny_true, y_pred = [], []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        X_batch = X_batch.to(device)\n        output = final_model(X_batch)\n        preds = output.argmax(dim=1).cpu().numpy()\n        y_pred.extend(preds)\n        y_true.extend(y_batch.numpy())\n\nfrom sklearn.metrics import accuracy_score, classification_report\n\nprint(\"üéØ Test Accuracy:\", accuracy_score(y_true, y_pred))\nprint(classification_report(y_true, y_pred, digits=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:55:14.627727Z","iopub.execute_input":"2025-05-06T16:55:14.628147Z","iopub.status.idle":"2025-05-06T16:55:16.085100Z","shell.execute_reply.started":"2025-05-06T16:55:14.628117Z","shell.execute_reply":"2025-05-06T16:55:16.084139Z"}},"outputs":[{"name":"stdout","text":"üéØ Test Accuracy: 0.9896341463414634\n              precision    recall  f1-score   support\n\n           0     0.9804    0.9939    0.9871      1309\n           1     0.9959    0.9868    0.9913      1971\n\n    accuracy                         0.9896      3280\n   macro avg     0.9882    0.9903    0.9892      3280\nweighted avg     0.9897    0.9896    0.9896      3280\n\n","output_type":"stream"}],"execution_count":60}]}