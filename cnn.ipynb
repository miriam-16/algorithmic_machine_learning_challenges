{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T14:35:33.979508Z",
     "iopub.status.busy": "2025-05-06T14:35:33.979102Z",
     "iopub.status.idle": "2025-05-06T14:35:33.990040Z",
     "shell.execute_reply": "2025-05-06T14:35:33.988923Z",
     "shell.execute_reply.started": "2025-05-06T14:35:33.979481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageDraw\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "from torchvision import utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T14:35:33.994438Z",
     "iopub.status.busy": "2025-05-06T14:35:33.994053Z",
     "iopub.status.idle": "2025-05-06T14:35:34.045925Z",
     "shell.execute_reply": "2025-05-06T14:35:34.044975Z",
     "shell.execute_reply.started": "2025-05-06T14:35:33.994411Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id  has_cactus\n",
      "0  0004be2cfeaba1c0361d39e2b000257b.jpg           1\n",
      "1  000c8a36845c0208e833c79c1bffedd1.jpg           1\n",
      "2  000d1e9a533f62e55c289303b072733d.jpg           1\n",
      "3  0011485b40695e9138e92d0b3fb55128.jpg           1\n",
      "4  0014d7a11e90b62848904c1418fc8cf2.jpg           1\n"
     ]
    }
   ],
   "source": [
    "path = \"/kaggle/input/aml-challenge1\"\n",
    "import pandas as pd\n",
    "\n",
    "labels_df = pd.read_csv(path+'/train.csv')  # Adjust filename\n",
    "print(labels_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T14:35:34.048248Z",
     "iopub.status.busy": "2025-05-06T14:35:34.047869Z",
     "iopub.status.idle": "2025-05-06T14:35:34.063187Z",
     "shell.execute_reply": "2025-05-06T14:35:34.062243Z",
     "shell.execute_reply.started": "2025-05-06T14:35:34.048224Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17500, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>has_cactus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, has_cactus]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(labels_df.shape)\n",
    "labels_df[labels_df.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T14:35:34.064758Z",
     "iopub.status.busy": "2025-05-06T14:35:34.064342Z",
     "iopub.status.idle": "2025-05-06T14:35:34.076882Z",
     "shell.execute_reply": "2025-05-06T14:35:34.075783Z",
     "shell.execute_reply.started": "2025-05-06T14:35:34.064715Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "has_cactus\n",
       "1    13136\n",
       "0     4364\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df['has_cactus'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T14:35:34.080235Z",
     "iopub.status.busy": "2025-05-06T14:35:34.079831Z",
     "iopub.status.idle": "2025-05-06T14:35:34.100169Z",
     "shell.execute_reply": "2025-05-06T14:35:34.099165Z",
     "shell.execute_reply.started": "2025-05-06T14:35:34.080207Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class pytorch_data(Dataset):\n",
    "    \n",
    "    def __init__(self, data_dir, transform, data_type=\"train\"):\n",
    "        # Get Image File Names\n",
    "        cdm_data = os.path.join(data_dir, data_type)\n",
    "        file_names = os.listdir(cdm_data)\n",
    "\n",
    "        all_image_paths = [os.path.join(cdm_data, f) for f in file_names if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "        print(f\"Found {len(all_image_paths)} images in directory.\")\n",
    "        print(f\"Sample filenames: {all_image_paths[:1]}\")\n",
    "\n",
    "        # Get Labels\n",
    "        labels_data = os.path.join(\"/kaggle/input/aml-challenge1/\", \"train.csv\")\n",
    "        labels_df = pd.read_csv(labels_data)\n",
    "\n",
    "        # Normalize index: remove extensions if present\n",
    "        labels_df['id'] = labels_df['id'].apply(lambda x: os.path.splitext(str(x))[0])\n",
    "        labels_df.set_index(\"id\", inplace=True)\n",
    "\n",
    "        print(f\"Labels dataframe length: {len(labels_df)}\")\n",
    "\n",
    "        # Extract only valid images (that have a label)\n",
    "        valid_filenames = []\n",
    "        labels = []\n",
    "\n",
    "        for f in all_image_paths:\n",
    "            filename = os.path.basename(f)  # get only file name\n",
    "            image_id = os.path.splitext(filename)[0]  # remove extension\n",
    "\n",
    "            if image_id in labels_df.index:\n",
    "                valid_filenames.append(f)\n",
    "                labels.append(labels_df.loc[image_id].values[0])\n",
    "            else:\n",
    "                print(f\"Warning: image '{filename}' has no matching label in train.csv\")\n",
    "\n",
    "        self.full_filenames = valid_filenames\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f\"Valid image-label pairs: {len(self.full_filenames)}\")\n",
    "        print(f\"First few labels: {self.labels[:5]}\")\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.full_filenames)\n",
    "      \n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.full_filenames):\n",
    "            raise IndexError(f\"Index {idx} out of bounds for dataset of length {len(self.full_filenames)}\")\n",
    "\n",
    "        image = Image.open(self.full_filenames[idx])\n",
    "        image = self.transform(image)\n",
    "        return image, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T14:35:34.101632Z",
     "iopub.status.busy": "2025-05-06T14:35:34.101267Z",
     "iopub.status.idle": "2025-05-06T14:35:34.121132Z",
     "shell.execute_reply": "2025-05-06T14:35:34.119861Z",
     "shell.execute_reply.started": "2025-05-06T14:35:34.101587Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# define transformation that converts a PIL image into PyTorch tensors\n",
    "import torchvision.transforms as transforms\n",
    "data_transformer = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Resize((32,32))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T14:35:34.122706Z",
     "iopub.status.busy": "2025-05-06T14:35:34.122288Z",
     "iopub.status.idle": "2025-05-06T14:35:34.663493Z",
     "shell.execute_reply": "2025-05-06T14:35:34.662656Z",
     "shell.execute_reply.started": "2025-05-06T14:35:34.122661Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17500 images in directory.\n",
      "Sample filenames: ['/kaggle/input/aml-challenge1/train/train/5d3a7d32516a92cc0dc8c52af515eaa4.jpg']\n",
      "Labels dataframe length: 17500\n",
      "Valid image-label pairs: 17500\n",
      "First few labels: [1, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Define an object of the custom dataset for the train folder.\n",
    "data_dir = path+'/train/'\n",
    "img_dataset = pytorch_data(data_dir, data_transformer, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T14:35:34.665183Z",
     "iopub.status.busy": "2025-05-06T14:35:34.664780Z",
     "iopub.status.idle": "2025-05-06T14:35:34.670130Z",
     "shell.execute_reply": "2025-05-06T14:35:34.669232Z",
     "shell.execute_reply.started": "2025-05-06T14:35:34.665149Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# define transformation that converts a PIL image into PyTorch tensors\n",
    "data_transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((32, 32))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T14:35:34.671483Z",
     "iopub.status.busy": "2025-05-06T14:35:34.671160Z",
     "iopub.status.idle": "2025-05-06T14:35:34.699184Z",
     "shell.execute_reply": "2025-05-06T14:35:34.697999Z",
     "shell.execute_reply.started": "2025-05-06T14:35:34.671453Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32]) tensor(0.2667) tensor(0.8627)\n"
     ]
    }
   ],
   "source": [
    "# Test a sample\n",
    "img, label = img_dataset[10]\n",
    "print(img.shape, torch.min(img), torch.max(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA AUGMENTATION CHE VA A RADDOPPIARE IL NUMERO DI SAMPLE DELLA CLASSE 0 AVENDO ALLA FINE CHE I SAMPLE DELLA CLASSE 0 SONO I 2/3 DEI SAMPLE DELLA CLASSE 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T14:45:47.390852Z",
     "iopub.status.busy": "2025-05-06T14:45:47.390514Z",
     "iopub.status.idle": "2025-05-06T14:46:49.264681Z",
     "shell.execute_reply": "2025-05-06T14:46:49.263678Z",
     "shell.execute_reply.started": "2025-05-06T14:45:47.390826Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4364/4364 [00:10<00:00, 399.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# Aumentiamo le immagini della classe 0\n",
    "from torchvision.transforms import RandomRotation, ToTensor, Resize\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Trasformazione per augmentare\n",
    "augment_transform = transforms.Compose([\n",
    "    RandomRotation(degrees=10),\n",
    "    Resize((32, 32)),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "# Trova solo immagini con etichetta 0\n",
    "images_class0 = [i for i in range(len(img_dataset)) if img_dataset.labels[i] == 0]\n",
    "\n",
    "# Duplichiamo queste immagini con trasformazione\n",
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "\n",
    "for idx in tqdm(images_class0):\n",
    "    img_path = img_dataset.full_filenames[idx]\n",
    "    img = Image.open(img_path)\n",
    "    augmented_img = augment_transform(img)\n",
    "    augmented_images.append(augmented_img)\n",
    "    augmented_labels.append(0)\n",
    "\n",
    "# Stack immagini originali\n",
    "original_images = [img_dataset[i][0] for i in range(len(img_dataset))]\n",
    "original_labels = [img_dataset[i][1] for i in range(len(img_dataset))]\n",
    "\n",
    "# Combina immagini originali + augmentate\n",
    "all_images = torch.stack(original_images + augmented_images)\n",
    "all_labels = torch.tensor(original_labels + augmented_labels)\n",
    "\n",
    "# Nuovo Dataset custom con dati augmentati\n",
    "class AugmentedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "# Sostituisci img_dataset con quello nuovo\n",
    "img_dataset = AugmentedDataset(all_images, all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***DIVISIONE NEI TRE SET TRAIN, VALIDATION, TEST DOPO AVER AUMENTATO I DATI DELLA CLASSE 0***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:16:22.595681Z",
     "iopub.status.busy": "2025-05-06T15:16:22.595389Z",
     "iopub.status.idle": "2025-05-06T15:16:23.146797Z",
     "shell.execute_reply": "2025-05-06T15:16:23.146037Z",
     "shell.execute_reply.started": "2025-05-06T15:16:22.595661Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset size: 15304\n",
      "validation dataset size: 3280\n",
      "test dataset size: 3280\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Estrai le etichette in modo sicuro\n",
    "labels = img_dataset.labels  # NON usare img_dataset[i][1]\n",
    "\n",
    "# Crea una lista di tutti gli indici\n",
    "all_indices = list(range(len(img_dataset)))\n",
    "\n",
    "# Split stratificato: Train (70%) e Temp (30%)\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    all_indices, test_size=0.3, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Estrai le label corrispondenti agli indici temporanei per secondo split\n",
    "temp_labels = [labels[i] for i in temp_idx]\n",
    "\n",
    "# Split stratificato: Validation (15%) e Test (15%) da temp\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx, test_size=0.5, stratify=temp_labels, random_state=42\n",
    ")\n",
    "\n",
    "# Crea i subset PyTorch\n",
    "train_ts = Subset(img_dataset, train_idx)\n",
    "val_ts = Subset(img_dataset, val_idx)\n",
    "test_ts = Subset(img_dataset, test_idx)\n",
    "\n",
    "# Visualizzazione\n",
    "print(\"train dataset size:\", len(train_ts))\n",
    "print(\"validation dataset size:\", len(val_ts))\n",
    "print(\"test dataset size:\", len(test_ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA LOADER FOR SPLITTING IMAGES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:37:15.570693Z",
     "iopub.status.busy": "2025-05-06T15:37:15.569972Z",
     "iopub.status.idle": "2025-05-06T15:37:15.576890Z",
     "shell.execute_reply": "2025-05-06T15:37:15.575672Z",
     "shell.execute_reply.started": "2025-05-06T15:37:15.570657Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Training DataLoader\n",
    "train_dl = DataLoader(train_ts,\n",
    "                      batch_size=64, \n",
    "                      shuffle=True)\n",
    "\n",
    "# Validation DataLoader\n",
    "val_dl = DataLoader(val_ts,\n",
    "                    batch_size=64,\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEFINIZIONE DEL MODELLO CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:37:18.604502Z",
     "iopub.status.busy": "2025-05-06T15:37:18.603580Z",
     "iopub.status.idle": "2025-05-06T15:37:18.617237Z",
     "shell.execute_reply": "2025-05-06T15:37:18.616202Z",
     "shell.execute_reply.started": "2025-05-06T15:37:18.604469Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def findConv2dOutShape(hin,win,conv,pool=2):\n",
    "    # get conv arguments\n",
    "    kernel_size=conv.kernel_size\n",
    "    stride=conv.stride\n",
    "    padding=conv.padding\n",
    "    dilation=conv.dilation\n",
    "\n",
    "    hout=np.floor((hin+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0]+1)\n",
    "    wout=np.floor((win+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1]+1)\n",
    "\n",
    "    if pool:\n",
    "        hout/=pool\n",
    "        wout/=pool\n",
    "    return int(hout),int(wout)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Neural Network\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    # Network Initialisation\n",
    "    def __init__(self, params):\n",
    "        \n",
    "        super(Network, self).__init__()\n",
    "    \n",
    "        Cin,Hin,Win=params[\"shape_in\"]\n",
    "        init_f=params[\"initial_filters\"] \n",
    "        num_fc1=params[\"num_fc1\"]  \n",
    "        num_classes=params[\"num_classes\"] \n",
    "        self.dropout_rate=params[\"dropout_rate\"] \n",
    "        \n",
    "        # Convolution Layers\n",
    "        self.conv1 = nn.Conv2d(Cin, init_f, kernel_size=3, padding=1)\n",
    "        h,w=findConv2dOutShape(Hin,Win,self.conv1)\n",
    "        self.conv2 = nn.Conv2d(init_f, 2*init_f, kernel_size=3,padding=1)\n",
    "        h,w=findConv2dOutShape(h,w,self.conv2)\n",
    "        self.conv3 = nn.Conv2d(2*init_f, 4*init_f, kernel_size=3, padding=1)\n",
    "        h,w=findConv2dOutShape(h,w,self.conv3)\n",
    "        \n",
    "        # compute the flatten size\n",
    "        self.num_flatten=h*w*4*init_f   #4*8 -> 32\n",
    "        self.fc1 = nn.Linear(self.num_flatten, num_fc1)\n",
    "        self.fc2 = nn.Linear(num_fc1, num_classes)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self,X):\n",
    "        # Convolution & Pool Layers\n",
    "        X = F.relu(self.conv1(X));\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv3(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "\n",
    "        b = X.shape[0]\n",
    "        X = X.view(b,-1) #torch appiattisce su tutti i canali, tranne il primo che corrisponde alla batch size\n",
    "        \n",
    "        X = F.relu(self.fc1(X))\n",
    "        X=F.dropout(X, self.dropout_rate)\n",
    "        X = self.fc2(X)\n",
    "        return F.log_softmax(X, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PARAMETRI DEL MODELLO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:37:23.015384Z",
     "iopub.status.busy": "2025-05-06T15:37:23.015084Z",
     "iopub.status.idle": "2025-05-06T15:37:23.030743Z",
     "shell.execute_reply": "2025-05-06T15:37:23.029959Z",
     "shell.execute_reply.started": "2025-05-06T15:37:23.015363Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Neural Network Predefined Parameters\n",
    "params_model={\n",
    "        \"shape_in\": (3,32,32), \n",
    "        \"initial_filters\": 8,    \n",
    "        \"num_fc1\": 100,\n",
    "        \"dropout_rate\": 0.25,\n",
    "        \"num_classes\": 2}\n",
    "\n",
    "# Create instantiation of Network class\n",
    "cnn_model = Network(params_model)\n",
    "\n",
    "# define computation hardware approach (GPU/CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = cnn_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRID SEARCH PER IL TUNING DEI PARAMETRI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:48:48.008522Z",
     "iopub.status.busy": "2025-05-06T15:48:48.007366Z",
     "iopub.status.idle": "2025-05-06T16:45:49.961193Z",
     "shell.execute_reply": "2025-05-06T16:45:49.959996Z",
     "shell.execute_reply.started": "2025-05-06T15:48:48.008487Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.001, Dropout: 0.25, Filters: 8, FC1: 64 -> Val Accuracy: 0.9637\n",
      "LR: 0.001, Dropout: 0.25, Filters: 8, FC1: 100 -> Val Accuracy: 0.9387\n",
      "LR: 0.001, Dropout: 0.25, Filters: 8, FC1: 128 -> Val Accuracy: 0.9619\n",
      "LR: 0.001, Dropout: 0.25, Filters: 16, FC1: 64 -> Val Accuracy: 0.9695\n",
      "LR: 0.001, Dropout: 0.25, Filters: 16, FC1: 100 -> Val Accuracy: 0.9793\n",
      "LR: 0.001, Dropout: 0.25, Filters: 16, FC1: 128 -> Val Accuracy: 0.9689\n",
      "LR: 0.001, Dropout: 0.25, Filters: 32, FC1: 64 -> Val Accuracy: 0.9823\n",
      "LR: 0.001, Dropout: 0.25, Filters: 32, FC1: 100 -> Val Accuracy: 0.9811\n",
      "LR: 0.001, Dropout: 0.25, Filters: 32, FC1: 128 -> Val Accuracy: 0.9817\n",
      "LR: 0.001, Dropout: 0.4, Filters: 8, FC1: 64 -> Val Accuracy: 0.9610\n",
      "LR: 0.001, Dropout: 0.4, Filters: 8, FC1: 100 -> Val Accuracy: 0.9622\n",
      "LR: 0.001, Dropout: 0.4, Filters: 8, FC1: 128 -> Val Accuracy: 0.9649\n",
      "LR: 0.001, Dropout: 0.4, Filters: 16, FC1: 64 -> Val Accuracy: 0.9540\n",
      "LR: 0.001, Dropout: 0.4, Filters: 16, FC1: 100 -> Val Accuracy: 0.9726\n",
      "LR: 0.001, Dropout: 0.4, Filters: 16, FC1: 128 -> Val Accuracy: 0.9488\n",
      "LR: 0.001, Dropout: 0.4, Filters: 32, FC1: 64 -> Val Accuracy: 0.9601\n",
      "LR: 0.001, Dropout: 0.4, Filters: 32, FC1: 100 -> Val Accuracy: 0.9646\n",
      "LR: 0.001, Dropout: 0.4, Filters: 32, FC1: 128 -> Val Accuracy: 0.9683\n",
      "LR: 0.001, Dropout: 0.5, Filters: 8, FC1: 64 -> Val Accuracy: 0.9601\n",
      "LR: 0.001, Dropout: 0.5, Filters: 8, FC1: 100 -> Val Accuracy: 0.9369\n",
      "LR: 0.001, Dropout: 0.5, Filters: 8, FC1: 128 -> Val Accuracy: 0.9643\n",
      "LR: 0.001, Dropout: 0.5, Filters: 16, FC1: 64 -> Val Accuracy: 0.9637\n",
      "LR: 0.001, Dropout: 0.5, Filters: 16, FC1: 100 -> Val Accuracy: 0.9348\n",
      "LR: 0.001, Dropout: 0.5, Filters: 16, FC1: 128 -> Val Accuracy: 0.9790\n",
      "LR: 0.001, Dropout: 0.5, Filters: 32, FC1: 64 -> Val Accuracy: 0.9774\n",
      "LR: 0.001, Dropout: 0.5, Filters: 32, FC1: 100 -> Val Accuracy: 0.9787\n",
      "LR: 0.001, Dropout: 0.5, Filters: 32, FC1: 128 -> Val Accuracy: 0.9564\n",
      "LR: 0.005, Dropout: 0.25, Filters: 8, FC1: 64 -> Val Accuracy: 0.9601\n",
      "LR: 0.005, Dropout: 0.25, Filters: 8, FC1: 100 -> Val Accuracy: 0.9741\n",
      "LR: 0.005, Dropout: 0.25, Filters: 8, FC1: 128 -> Val Accuracy: 0.9604\n",
      "LR: 0.005, Dropout: 0.25, Filters: 16, FC1: 64 -> Val Accuracy: 0.9729\n",
      "LR: 0.005, Dropout: 0.25, Filters: 16, FC1: 100 -> Val Accuracy: 0.9689\n",
      "LR: 0.005, Dropout: 0.25, Filters: 16, FC1: 128 -> Val Accuracy: 0.9488\n",
      "LR: 0.005, Dropout: 0.25, Filters: 32, FC1: 64 -> Val Accuracy: 0.9671\n",
      "LR: 0.005, Dropout: 0.25, Filters: 32, FC1: 100 -> Val Accuracy: 0.9759\n",
      "LR: 0.005, Dropout: 0.25, Filters: 32, FC1: 128 -> Val Accuracy: 0.9732\n",
      "LR: 0.005, Dropout: 0.4, Filters: 8, FC1: 64 -> Val Accuracy: 0.9613\n",
      "LR: 0.005, Dropout: 0.4, Filters: 8, FC1: 100 -> Val Accuracy: 0.9677\n",
      "LR: 0.005, Dropout: 0.4, Filters: 8, FC1: 128 -> Val Accuracy: 0.9582\n",
      "LR: 0.005, Dropout: 0.4, Filters: 16, FC1: 64 -> Val Accuracy: 0.9701\n",
      "LR: 0.005, Dropout: 0.4, Filters: 16, FC1: 100 -> Val Accuracy: 0.9738\n",
      "LR: 0.005, Dropout: 0.4, Filters: 16, FC1: 128 -> Val Accuracy: 0.9780\n",
      "LR: 0.005, Dropout: 0.4, Filters: 32, FC1: 64 -> Val Accuracy: 0.9665\n",
      "LR: 0.005, Dropout: 0.4, Filters: 32, FC1: 100 -> Val Accuracy: 0.9759\n",
      "LR: 0.005, Dropout: 0.4, Filters: 32, FC1: 128 -> Val Accuracy: 0.9762\n",
      "LR: 0.005, Dropout: 0.5, Filters: 8, FC1: 64 -> Val Accuracy: 0.9735\n",
      "LR: 0.005, Dropout: 0.5, Filters: 8, FC1: 100 -> Val Accuracy: 0.9659\n",
      "LR: 0.005, Dropout: 0.5, Filters: 8, FC1: 128 -> Val Accuracy: 0.9595\n",
      "LR: 0.005, Dropout: 0.5, Filters: 16, FC1: 64 -> Val Accuracy: 0.9186\n",
      "LR: 0.005, Dropout: 0.5, Filters: 16, FC1: 100 -> Val Accuracy: 0.9168\n",
      "LR: 0.005, Dropout: 0.5, Filters: 16, FC1: 128 -> Val Accuracy: 0.9713\n",
      "LR: 0.005, Dropout: 0.5, Filters: 32, FC1: 64 -> Val Accuracy: 0.8759\n",
      "LR: 0.005, Dropout: 0.5, Filters: 32, FC1: 100 -> Val Accuracy: 0.9704\n",
      "LR: 0.005, Dropout: 0.5, Filters: 32, FC1: 128 -> Val Accuracy: 0.9689\n",
      "LR: 0.01, Dropout: 0.25, Filters: 8, FC1: 64 -> Val Accuracy: 0.6006\n",
      "LR: 0.01, Dropout: 0.25, Filters: 8, FC1: 100 -> Val Accuracy: 0.9369\n",
      "LR: 0.01, Dropout: 0.25, Filters: 8, FC1: 128 -> Val Accuracy: 0.9591\n",
      "LR: 0.01, Dropout: 0.25, Filters: 16, FC1: 64 -> Val Accuracy: 0.9326\n",
      "LR: 0.01, Dropout: 0.25, Filters: 16, FC1: 100 -> Val Accuracy: 0.9515\n",
      "LR: 0.01, Dropout: 0.25, Filters: 16, FC1: 128 -> Val Accuracy: 0.6006\n",
      "LR: 0.01, Dropout: 0.25, Filters: 32, FC1: 64 -> Val Accuracy: 0.9576\n",
      "LR: 0.01, Dropout: 0.25, Filters: 32, FC1: 100 -> Val Accuracy: 0.9625\n",
      "LR: 0.01, Dropout: 0.25, Filters: 32, FC1: 128 -> Val Accuracy: 0.9601\n",
      "LR: 0.01, Dropout: 0.4, Filters: 8, FC1: 64 -> Val Accuracy: 0.9430\n",
      "LR: 0.01, Dropout: 0.4, Filters: 8, FC1: 100 -> Val Accuracy: 0.7591\n",
      "LR: 0.01, Dropout: 0.4, Filters: 8, FC1: 128 -> Val Accuracy: 0.9503\n",
      "LR: 0.01, Dropout: 0.4, Filters: 16, FC1: 64 -> Val Accuracy: 0.9485\n",
      "LR: 0.01, Dropout: 0.4, Filters: 16, FC1: 100 -> Val Accuracy: 0.9622\n",
      "LR: 0.01, Dropout: 0.4, Filters: 16, FC1: 128 -> Val Accuracy: 0.6006\n",
      "LR: 0.01, Dropout: 0.4, Filters: 32, FC1: 64 -> Val Accuracy: 0.9476\n",
      "LR: 0.01, Dropout: 0.4, Filters: 32, FC1: 100 -> Val Accuracy: 0.9549\n",
      "LR: 0.01, Dropout: 0.4, Filters: 32, FC1: 128 -> Val Accuracy: 0.9610\n",
      "LR: 0.01, Dropout: 0.5, Filters: 8, FC1: 64 -> Val Accuracy: 0.9357\n",
      "LR: 0.01, Dropout: 0.5, Filters: 8, FC1: 100 -> Val Accuracy: 0.9631\n",
      "LR: 0.01, Dropout: 0.5, Filters: 8, FC1: 128 -> Val Accuracy: 0.6006\n",
      "LR: 0.01, Dropout: 0.5, Filters: 16, FC1: 64 -> Val Accuracy: 0.7591\n",
      "LR: 0.01, Dropout: 0.5, Filters: 16, FC1: 100 -> Val Accuracy: 0.9549\n",
      "LR: 0.01, Dropout: 0.5, Filters: 16, FC1: 128 -> Val Accuracy: 0.9622\n",
      "LR: 0.01, Dropout: 0.5, Filters: 32, FC1: 64 -> Val Accuracy: 0.9268\n",
      "LR: 0.01, Dropout: 0.5, Filters: 32, FC1: 100 -> Val Accuracy: 0.9448\n",
      "LR: 0.01, Dropout: 0.5, Filters: 32, FC1: 128 -> Val Accuracy: 0.9430\n",
      "🏆 Best Accuracy: 0.9823 with params {'lr': 0.001, 'dropout_rate': 0.25, 'initial_filters': 32, 'num_fc1': 64}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 🔧 Hyperparameter grid\n",
    "lr_list = [0.001, 0.005, 0.01]\n",
    "dropout_list = [0.25, 0.4, 0.5]\n",
    "filters_list = [8, 16, 32]\n",
    "fc1_list = [64, 100, 128]\n",
    "\n",
    "# 📦 Funzione per addestramento + validazione\n",
    "def train_and_evaluate(model, train_dl, val_dl, lr, device, epochs=5):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for X_batch, y_batch in train_dl:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_dl:\n",
    "            X_batch = X_batch.to(device)\n",
    "            output = model(X_batch)\n",
    "            preds = output.argmax(dim=1).cpu().numpy()\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(y_batch.numpy())\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    return acc\n",
    "\n",
    "# 🚀 Grid Search\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "best_acc = 0\n",
    "best_params = {}\n",
    "\n",
    "for lr in lr_list:\n",
    "    for dropout in dropout_list:\n",
    "        for filters in filters_list:\n",
    "            for fc1 in fc1_list:\n",
    "                params_model = {\n",
    "                    \"shape_in\": (3, 32, 32),\n",
    "                    \"initial_filters\": filters,\n",
    "                    \"num_fc1\": fc1,\n",
    "                    \"dropout_rate\": dropout,\n",
    "                    \"num_classes\": 2\n",
    "                }\n",
    "\n",
    "                model = Network(params_model)\n",
    "                acc = train_and_evaluate(model, train_dl, val_dl, lr, device, epochs=5)\n",
    "\n",
    "                print(f\"LR: {lr}, Dropout: {dropout}, Filters: {filters}, FC1: {fc1} -> Val Accuracy: {acc:.4f}\")\n",
    "\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    best_params = {\n",
    "                        \"lr\": lr,\n",
    "                        \"dropout_rate\": dropout,\n",
    "                        \"initial_filters\": filters,\n",
    "                        \"num_fc1\": fc1\n",
    "                    }\n",
    "\n",
    "print(f\"🏆 Best Accuracy: {best_acc:.4f} with params {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USO I BEST IPERPARAMETRI SUL TEST**\n",
    "\n",
    "🏆 Best Accuracy: 0.9823 with params {'lr': 0.001, 'dropout_rate': 0.25, 'initial_filters': 32, 'num_fc1': 64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T16:52:01.659279Z",
     "iopub.status.busy": "2025-05-06T16:52:01.658895Z",
     "iopub.status.idle": "2025-05-06T16:52:01.669310Z",
     "shell.execute_reply": "2025-05-06T16:52:01.668390Z",
     "shell.execute_reply.started": "2025-05-06T16:52:01.659254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'lr': 0.001,\n",
    "    'dropout_rate': 0.25,\n",
    "    'initial_filters': 32,\n",
    "    'num_fc1': 64\n",
    "}\n",
    "\n",
    "# Costruzione del modello finale\n",
    "final_params = {\n",
    "    \"shape_in\": (3, 32, 32),\n",
    "    \"initial_filters\": best_params[\"initial_filters\"],\n",
    "    \"num_fc1\": best_params[\"num_fc1\"],\n",
    "    \"dropout_rate\": best_params[\"dropout_rate\"],\n",
    "    \"num_classes\": 2\n",
    "}\n",
    "\n",
    "final_model = Network(final_params).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T16:52:05.043121Z",
     "iopub.status.busy": "2025-05-06T16:52:05.042754Z",
     "iopub.status.idle": "2025-05-06T16:55:08.665262Z",
     "shell.execute_reply": "2025-05-06T16:55:08.664410Z",
     "shell.execute_reply.started": "2025-05-06T16:52:05.043092Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 65.1843\n",
      "Epoch 2 - Loss: 25.6979\n",
      "Epoch 3 - Loss: 17.9817\n",
      "Epoch 4 - Loss: 16.0125\n",
      "Epoch 5 - Loss: 12.0859\n",
      "Epoch 6 - Loss: 10.3995\n",
      "Epoch 7 - Loss: 7.9387\n",
      "Epoch 8 - Loss: 7.8818\n",
      "Epoch 9 - Loss: 7.4810\n",
      "Epoch 10 - Loss: 5.5941\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "# Combina train + validation set\n",
    "full_train_set = ConcatDataset([train_ts, val_ts])\n",
    "full_train_loader = DataLoader(full_train_set, batch_size=64, shuffle=True)\n",
    "\n",
    "# Ottimizzatore e loss\n",
    "optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params[\"lr\"])\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Training\n",
    "final_model.train()\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in full_train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = final_model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {running_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T16:55:14.628147Z",
     "iopub.status.busy": "2025-05-06T16:55:14.627727Z",
     "iopub.status.idle": "2025-05-06T16:55:16.085100Z",
     "shell.execute_reply": "2025-05-06T16:55:16.084139Z",
     "shell.execute_reply.started": "2025-05-06T16:55:14.628117Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Test Accuracy: 0.9896341463414634\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9804    0.9939    0.9871      1309\n",
      "           1     0.9959    0.9868    0.9913      1971\n",
      "\n",
      "    accuracy                         0.9896      3280\n",
      "   macro avg     0.9882    0.9903    0.9892      3280\n",
      "weighted avg     0.9897    0.9896    0.9896      3280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_ts, batch_size=64, shuffle=False)\n",
    "\n",
    "# Valutazione\n",
    "final_model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        output = final_model(X_batch)\n",
    "        preds = output.argmax(dim=1).cpu().numpy()\n",
    "        y_pred.extend(preds)\n",
    "        y_true.extend(y_batch.numpy())\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"🎯 Test Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percorso delle immagini di test\n",
    "test_path = \"/kaggle/input/aml-challenge1/test/test\"\n",
    "test_image_paths = [os.path.join(test_path, f) for f in os.listdir(test_path) if f.endswith(\".jpg\")]\n",
    "\n",
    "# Trasformazioni da usare (uguali al training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),   # Adatta alla dimensione usata in training\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Esegui il modello sul test set\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for path in test_image_paths:\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        output = model(image)\n",
    "        pred = torch.argmax(output, dim=1).item()\n",
    "\n",
    "        filename = os.path.basename(path)\n",
    "        predictions.append((filename, pred))\n",
    "\n",
    "# Salva in CSV\n",
    "submission_df = pd.DataFrame(predictions, columns=[\"id\", \"has_cactus\"])\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Leggi il file CSV con le predizioni\n",
    "submission_df = pd.read_csv(\"submission.csv\")\n",
    "\n",
    "# Conta le occorrenze di ciascuna classe\n",
    "class_counts = submission_df[\"has_cactus\"].value_counts()\n",
    "\n",
    "# Stampa i risultati\n",
    "print(\"Conteggio predizioni per classe:\")\n",
    "print(class_counts)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7341610,
     "sourceId": 11696839,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
