{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11717605,"sourceType":"datasetVersion","datasetId":7355441}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nimport copy\nimport os\nimport torch\nfrom PIL import Image\nfrom PIL import Image, ImageDraw\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nfrom torch.utils.data import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch.nn as nn\nfrom torchvision import utils\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2025-05-08T13:04:45.652827Z","iopub.execute_input":"2025-05-08T13:04:45.653186Z","iopub.status.idle":"2025-05-08T13:04:55.447266Z","shell.execute_reply.started":"2025-05-08T13:04:45.653160Z","shell.execute_reply":"2025-05-08T13:04:55.446124Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"path = \"/kaggle/input/aml-challenge-1/dataset\"\nimport pandas as pd\n\nlabels_df = pd.read_csv(path+'/train.csv')  # Adjust filename\nprint(labels_df.head())","metadata":{"execution":{"iopub.status.busy":"2025-05-08T13:04:55.448765Z","iopub.execute_input":"2025-05-08T13:04:55.449431Z","iopub.status.idle":"2025-05-08T13:04:55.535861Z","shell.execute_reply.started":"2025-05-08T13:04:55.449405Z","shell.execute_reply":"2025-05-08T13:04:55.534564Z"},"trusted":true},"outputs":[{"name":"stdout","text":"                                     id  has_cactus\n0  0004be2cfeaba1c0361d39e2b000257b.jpg           1\n1  000c8a36845c0208e833c79c1bffedd1.jpg           1\n2  000d1e9a533f62e55c289303b072733d.jpg           1\n3  0011485b40695e9138e92d0b3fb55128.jpg           1\n4  0014d7a11e90b62848904c1418fc8cf2.jpg           1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"print(labels_df.shape)\nlabels_df[labels_df.duplicated(keep=False)]","metadata":{"execution":{"iopub.status.busy":"2025-05-08T13:04:55.536937Z","iopub.execute_input":"2025-05-08T13:04:55.537254Z","iopub.status.idle":"2025-05-08T13:04:55.572300Z","shell.execute_reply.started":"2025-05-08T13:04:55.537233Z","shell.execute_reply":"2025-05-08T13:04:55.571377Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(17500, 2)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [id, has_cactus]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>has_cactus</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"labels_df['has_cactus'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2025-05-08T13:04:55.574235Z","iopub.execute_input":"2025-05-08T13:04:55.574472Z","iopub.status.idle":"2025-05-08T13:04:55.585722Z","shell.execute_reply.started":"2025-05-08T13:04:55.574455Z","shell.execute_reply":"2025-05-08T13:04:55.584341Z"},"trusted":true},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"has_cactus\n1    13136\n0     4364\nName: count, dtype: int64"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import torch\ntorch.manual_seed(0)\n\nfrom torch.utils.data import Dataset\nimport os\nimport pandas as pd\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\nclass pytorch_data(Dataset):\n    \n    def __init__(self, data_dir, transform, data_type=\"train\"):\n        # Get Image File Names\n        cdm_data = os.path.join(data_dir, data_type)\n        file_names = os.listdir(cdm_data)\n\n        all_image_paths = [os.path.join(cdm_data, f) for f in file_names if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n\n        print(f\"Found {len(all_image_paths)} images in directory.\")\n        print(f\"Sample filenames: {all_image_paths[:1]}\")\n\n        # Get Labels\n        labels_data = os.path.join(\"/kaggle/input/aml-challenge-1/dataset/\", \"train.csv\")\n        labels_df = pd.read_csv(labels_data)\n\n        # Normalize index: remove extensions if present\n        labels_df['id'] = labels_df['id'].apply(lambda x: os.path.splitext(str(x))[0])\n        labels_df.set_index(\"id\", inplace=True)\n\n        print(f\"Labels dataframe length: {len(labels_df)}\")\n\n        # Extract only valid images (that have a label)\n        valid_filenames = []\n        labels = []\n\n        for f in all_image_paths:\n            filename = os.path.basename(f)  # get only file name\n            image_id = os.path.splitext(filename)[0]  # remove extension\n\n            if image_id in labels_df.index:\n                valid_filenames.append(f)\n                labels.append(labels_df.loc[image_id].values[0])\n            else:\n                print(f\"Warning: image '{filename}' has no matching label in train.csv\")\n\n        self.full_filenames = valid_filenames\n        self.labels = labels\n        self.transform = transform\n\n        print(f\"Valid image-label pairs: {len(self.full_filenames)}\")\n        print(f\"First few labels: {self.labels[:5]}\")\n      \n    def __len__(self):\n        return len(self.full_filenames)\n      \n    def __getitem__(self, idx):\n        if idx >= len(self.full_filenames):\n            raise IndexError(f\"Index {idx} out of bounds for dataset of length {len(self.full_filenames)}\")\n\n        image = Image.open(self.full_filenames[idx])\n        image = self.transform(image)\n        return image, self.labels[idx]","metadata":{"execution":{"iopub.status.busy":"2025-05-08T13:04:55.586633Z","iopub.execute_input":"2025-05-08T13:04:55.586891Z","iopub.status.idle":"2025-05-08T13:04:55.606016Z","shell.execute_reply.started":"2025-05-08T13:04:55.586871Z","shell.execute_reply":"2025-05-08T13:04:55.604986Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# define transformation that converts a PIL image into PyTorch tensors\nimport torchvision.transforms as transforms\ndata_transformer = transforms.Compose([transforms.ToTensor(),\n                                       transforms.Resize((32,32))])","metadata":{"execution":{"iopub.status.busy":"2025-05-08T13:04:55.607367Z","iopub.execute_input":"2025-05-08T13:04:55.607675Z","iopub.status.idle":"2025-05-08T13:04:55.615779Z","shell.execute_reply.started":"2025-05-08T13:04:55.607653Z","shell.execute_reply":"2025-05-08T13:04:55.614752Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Define an object of the custom dataset for the train folder.\ndata_dir = path+'/train/'\nimg_dataset = pytorch_data(data_dir, data_transformer, \"train\")","metadata":{"execution":{"iopub.status.busy":"2025-05-08T13:04:55.617000Z","iopub.execute_input":"2025-05-08T13:04:55.617505Z","iopub.status.idle":"2025-05-08T13:04:56.892316Z","shell.execute_reply.started":"2025-05-08T13:04:55.617473Z","shell.execute_reply":"2025-05-08T13:04:56.891215Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Found 17500 images in directory.\nSample filenames: ['/kaggle/input/aml-challenge-1/dataset/train/train/5d3a7d32516a92cc0dc8c52af515eaa4.jpg']\nLabels dataframe length: 17500\nValid image-label pairs: 17500\nFirst few labels: [1, 0, 1, 1, 1]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# define transformation that converts a PIL image into PyTorch tensors\ndata_transformer = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((32, 32))\n])","metadata":{"execution":{"iopub.status.busy":"2025-05-08T13:04:56.893240Z","iopub.execute_input":"2025-05-08T13:04:56.893470Z","iopub.status.idle":"2025-05-08T13:04:56.898493Z","shell.execute_reply.started":"2025-05-08T13:04:56.893451Z","shell.execute_reply":"2025-05-08T13:04:56.897527Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Test a sample\nimg, label = img_dataset[10]\nprint(img.shape, torch.min(img), torch.max(img))","metadata":{"execution":{"iopub.status.busy":"2025-05-08T13:04:56.899681Z","iopub.execute_input":"2025-05-08T13:04:56.900659Z","iopub.status.idle":"2025-05-08T13:04:57.075523Z","shell.execute_reply.started":"2025-05-08T13:04:56.900630Z","shell.execute_reply":"2025-05-08T13:04:57.074458Z"},"trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([3, 32, 32]) tensor(0.2667) tensor(0.8627)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"**DATA AUGMENTATION CHE VA A RADDOPPIARE IL NUMERO DI SAMPLE DELLA CLASSE 0 AVENDO ALLA FINE CHE I SAMPLE DELLA CLASSE 0 SONO I 2/3 DEI SAMPLE DELLA CLASSE 1**","metadata":{}},{"cell_type":"code","source":"# Aumentiamo le immagini della classe 0\nfrom torchvision.transforms import RandomRotation, ToTensor, Resize\nfrom tqdm import tqdm\n\n# Trasformazione per augmentare\naugment_transform = transforms.Compose([\n    RandomRotation(degrees=10),\n    Resize((32, 32)),\n    ToTensor()\n])\n\n# Trova solo immagini con etichetta 0\nimages_class0 = [i for i in range(len(img_dataset)) if img_dataset.labels[i] == 0]\n\n# Duplichiamo queste immagini con trasformazione\naugmented_images = []\naugmented_labels = []\n\nfor idx in tqdm(images_class0):\n    img_path = img_dataset.full_filenames[idx]\n    img = Image.open(img_path)\n    augmented_img = augment_transform(img)\n    augmented_images.append(augmented_img)\n    augmented_labels.append(0)\n\n# Stack immagini originali\noriginal_images = [img_dataset[i][0] for i in range(len(img_dataset))]\noriginal_labels = [img_dataset[i][1] for i in range(len(img_dataset))]\n\n# Combina immagini originali + augmentate\nall_images = torch.stack(original_images + augmented_images)\nall_labels = torch.tensor(original_labels + augmented_labels)\n\n# Nuovo Dataset custom con dati augmentati\nclass AugmentedDataset(torch.utils.data.Dataset):\n    def __init__(self, images, labels):\n        self.images = images\n        self.labels = labels\n    def __len__(self):\n        return len(self.images)\n    def __getitem__(self, idx):\n        return self.images[idx], self.labels[idx]\n\n# Sostituisci img_dataset con quello nuovo\nimg_dataset = AugmentedDataset(all_images, all_labels)","metadata":{"execution":{"iopub.status.busy":"2025-05-08T13:04:57.078462Z","iopub.execute_input":"2025-05-08T13:04:57.078771Z","iopub.status.idle":"2025-05-08T13:08:08.304969Z","shell.execute_reply.started":"2025-05-08T13:04:57.078748Z","shell.execute_reply":"2025-05-08T13:08:08.303860Z"},"trusted":true},"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4364/4364 [00:42<00:00, 101.93it/s]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"***DIVISIONE NEI TRE SET TRAIN, VALIDATION, TEST DOPO AVER AUMENTATO I DATI DELLA CLASSE 0***","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import Subset\n\n# Estrai le etichette in modo sicuro\nlabels = img_dataset.labels  # NON usare img_dataset[i][1]\n\n# Crea una lista di tutti gli indici\nall_indices = list(range(len(img_dataset)))\n\n# Split stratificato: Train (70%) e Temp (30%)\ntrain_idx, temp_idx = train_test_split(\n    all_indices, test_size=0.3, stratify=labels, random_state=42\n)\n\n# Estrai le label corrispondenti agli indici temporanei per secondo split\ntemp_labels = [labels[i] for i in temp_idx]\n\n# Split stratificato: Validation (15%) e Test (15%) da temp\nval_idx, test_idx = train_test_split(\n    temp_idx, test_size=0.5, stratify=temp_labels, random_state=42\n)\n\n# Crea i subset PyTorch\ntrain_ts = Subset(img_dataset, train_idx)\nval_ts = Subset(img_dataset, val_idx)\ntest_ts = Subset(img_dataset, test_idx)\n\n# Visualizzazione\nprint(\"train dataset size:\", len(train_ts))\nprint(\"validation dataset size:\", len(val_ts))\nprint(\"test dataset size:\", len(test_ts))","metadata":{"execution":{"iopub.status.busy":"2025-05-08T13:08:08.306007Z","iopub.execute_input":"2025-05-08T13:08:08.306291Z","iopub.status.idle":"2025-05-08T13:08:09.431322Z","shell.execute_reply.started":"2025-05-08T13:08:08.306269Z","shell.execute_reply":"2025-05-08T13:08:09.430167Z"},"trusted":true},"outputs":[{"name":"stdout","text":"train dataset size: 15304\nvalidation dataset size: 3280\ntest dataset size: 3280\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Training DataLoader\ntrain_dl = DataLoader(train_ts,\n                      batch_size=64, \n                      shuffle=True)\n\n# Validation DataLoader\nval_dl = DataLoader(val_ts,\n                    batch_size=64,\n                    shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T13:08:09.432606Z","iopub.execute_input":"2025-05-08T13:08:09.433250Z","iopub.status.idle":"2025-05-08T13:08:09.438993Z","shell.execute_reply.started":"2025-05-08T13:08:09.433208Z","shell.execute_reply":"2025-05-08T13:08:09.437982Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import torch\ntorch.manual_seed(0)\nfrom torch.utils.data import DataLoader\n\nfrom torch.utils.data import Dataset\nimport os\nimport pandas as pd\nfrom PIL import Image\nimport torchvision.transforms as transforms\nclass UnlabeledDataset(Dataset):\n    def __init__(self, data_dir, transform):\n        self.transform = transform\n        file_names = os.listdir(data_dir)\n        self.image_paths = [os.path.join(data_dir, f) for f in file_names\n                            if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n        print(f\"Found {len(self.image_paths)} unlabeled images in 'test'.\")\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx])\n        image = self.transform(image)\n        return image\n\n# Trasformazione coerente\ndata_transformer = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((32, 32))\n])\nprint(path)\n# Path alla cartella test (modifica se necessario)\nunlabeled_data_dir = path + '/test/test'\nunlabeled_dataset = UnlabeledDataset(unlabeled_data_dir, data_transformer)\nunlabeled_loader = DataLoader(unlabeled_dataset, batch_size=64, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T13:08:09.440066Z","iopub.execute_input":"2025-05-08T13:08:09.440480Z","iopub.status.idle":"2025-05-08T13:08:09.816156Z","shell.execute_reply.started":"2025-05-08T13:08:09.440455Z","shell.execute_reply":"2025-05-08T13:08:09.814839Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/aml-challenge-1/dataset\nFound 4000 unlabeled images in 'test'.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def findConv2dOutShape(hin,win,conv,pool=2):\n    # get conv arguments\n    kernel_size=conv.kernel_size\n    stride=conv.stride\n    padding=conv.padding\n    dilation=conv.dilation\n\n    hout=np.floor((hin+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0]+1)\n    wout=np.floor((win+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1]+1)\n\n    if pool:\n        hout/=pool\n        wout/=pool\n    return int(hout),int(wout)\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Neural Network\nclass Network(nn.Module):\n    \n    # Network Initialisation\n    def __init__(self, params):\n        \n        super(Network, self).__init__()\n    \n        Cin,Hin,Win=params[\"shape_in\"]\n        init_f=params[\"initial_filters\"] \n        num_fc1=params[\"num_fc1\"]  \n        num_classes=params[\"num_classes\"] \n        self.dropout_rate=params[\"dropout_rate\"] \n        \n        # Convolution Layers\n        self.conv1 = nn.Conv2d(Cin, init_f, kernel_size=3, padding=1)\n        h,w=findConv2dOutShape(Hin,Win,self.conv1)\n        self.conv2 = nn.Conv2d(init_f, 2*init_f, kernel_size=3,padding=1)\n        h,w=findConv2dOutShape(h,w,self.conv2)\n        self.conv3 = nn.Conv2d(2*init_f, 4*init_f, kernel_size=3, padding=1)\n        h,w=findConv2dOutShape(h,w,self.conv3)\n        \n        # compute the flatten size\n        self.num_flatten=h*w*4*init_f   #4*8 -> 32\n        self.fc1 = nn.Linear(self.num_flatten, num_fc1)\n        self.fc2 = nn.Linear(num_fc1, num_classes)\n\n    \n\n    def forward(self,X):\n        # Convolution & Pool Layers\n        X = F.relu(self.conv1(X));\n        X = F.max_pool2d(X, 2, 2)\n        X = F.relu(self.conv2(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = F.relu(self.conv3(X))\n        X = F.max_pool2d(X, 2, 2)\n\n        b = X.shape[0]\n        X = X.view(b,-1) #torch appiattisce su tutti i canali, tranne il primo che corrisponde alla batch size\n        \n        X = F.relu(self.fc1(X))\n        X=F.dropout(X, self.dropout_rate)\n        X = self.fc2(X)\n        return F.log_softmax(X, dim=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T13:08:09.817334Z","iopub.execute_input":"2025-05-08T13:08:09.817700Z","iopub.status.idle":"2025-05-08T13:08:09.829856Z","shell.execute_reply.started":"2025-05-08T13:08:09.817676Z","shell.execute_reply":"2025-05-08T13:08:09.828516Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import ConcatDataset, DataLoader\n\n# Best parameters (giÃ  forniti)\nbest_params = {\n    'lr': 0.001,\n    'dropout_rate': 0.25,\n    'initial_filters': 32,\n    'num_fc1': 64\n}\n# define computation hardware approach (GPU/CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# Costruzione del modello finale\nfinal_params = {\n    \"shape_in\": (3, 32, 32),\n    \"initial_filters\": best_params[\"initial_filters\"],\n    \"num_fc1\": best_params[\"num_fc1\"],\n    \"dropout_rate\": best_params[\"dropout_rate\"],\n    \"num_classes\": 2\n}\n\nfinal_model_CNN = Network(final_params).to(device)\n\n# Unione train + validation\nfull_train_set = ConcatDataset([train_ts, val_ts])\nfull_train_loader = DataLoader(full_train_set, batch_size=64, shuffle=True)\n\n# Ottimizzatore e funzione di perdita\noptimizer = torch.optim.Adam(final_model_CNN.parameters(), lr=best_params[\"lr\"])\ncriterion = nn.NLLLoss()\n\n# Training\nfinal_model_CNN.train()\nfor epoch in range(10):\n    running_loss = 0.0\n    for X_batch, y_batch in full_train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n        output = final_model_CNN(X_batch)\n        loss = criterion(output, y_batch)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    print(f\"Epoch {epoch+1} - Loss: {running_loss:.4f}\")\n\n# Validazione finale sul test set\ntest_loader = DataLoader(test_ts, batch_size=64, shuffle=False)\nfinal_model_CNN.eval()\ny_true, y_pred = [], []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        X_batch = X_batch.to(device)\n        output = final_model_CNN(X_batch)\n        preds = output.argmax(dim=1).cpu().numpy()\n        y_pred.extend(preds)\n        y_true.extend(y_batch.numpy())\n\nfrom sklearn.metrics import accuracy_score, classification_report\naccuracy_CNN=accuracy_score(y_true, y_pred)\nprint(\"ðŸŽ¯ Test Accuracy:\", accuracy_score(y_true, y_pred))\nprint(classification_report(y_true, y_pred, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T13:08:09.831377Z","iopub.execute_input":"2025-05-08T13:08:09.831684Z","iopub.status.idle":"2025-05-08T13:12:07.285155Z","shell.execute_reply.started":"2025-05-08T13:08:09.831652Z","shell.execute_reply":"2025-05-08T13:12:07.284014Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 - Loss: 63.9040\nEpoch 2 - Loss: 30.9451\nEpoch 3 - Loss: 21.3514\nEpoch 4 - Loss: 13.9194\nEpoch 5 - Loss: 10.8894\nEpoch 6 - Loss: 9.5821\nEpoch 7 - Loss: 8.6208\nEpoch 8 - Loss: 7.2201\nEpoch 9 - Loss: 6.1977\nEpoch 10 - Loss: 5.2063\nðŸŽ¯ Test Accuracy: 0.9929878048780488\n              precision    recall  f1-score   support\n\n           0     0.9879    0.9947    0.9912      1309\n           1     0.9964    0.9919    0.9942      1971\n\n    accuracy                         0.9930      3280\n   macro avg     0.9921    0.9933    0.9927      3280\nweighted avg     0.9930    0.9930    0.9930      3280\n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def predict_unlabeled(model, loader):\n    model.eval()\n    predictions = []\n\n    with torch.no_grad():\n        for X_batch in loader:\n            X_batch = X_batch.to(device)\n            output = model(X_batch)\n            preds = output.argmax(dim=1).cpu().numpy()\n            predictions.extend(preds)\n\n    return predictions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T13:12:07.286448Z","iopub.execute_input":"2025-05-08T13:12:07.286747Z","iopub.status.idle":"2025-05-08T13:12:07.292386Z","shell.execute_reply.started":"2025-05-08T13:12:07.286718Z","shell.execute_reply":"2025-05-08T13:12:07.291480Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w, b\n\ndef sigmoid(z):\n    y_head = 1/(1+np.exp(-z))\n    return y_head\n\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/x_train.shape[1]\n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost,gradients\n\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    for i in range(number_of_iterarion):\n        \n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        \n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 100 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n\ndef predict(w,b,x_test):\n    \n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n\n    dimension =  x_train.shape[0]\n    w,b = initialize_weights_and_bias(dimension)\n\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n    \n    print(\"Test Accuracy: {} %\".format(round(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100,2)))\n    print(\"Train Accuracy: {} %\".format(round(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100,2)))\n    acc= (round(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100,2))\n    return acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T13:12:07.293468Z","iopub.execute_input":"2025-05-08T13:12:07.293777Z","iopub.status.idle":"2025-05-08T13:12:07.318301Z","shell.execute_reply.started":"2025-05-08T13:12:07.293752Z","shell.execute_reply":"2025-05-08T13:12:07.316961Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import numpy as np\n\ndef subset_to_numpy(subset):\n    images = []\n    labels = []\n    for img, label in subset:\n        images.append(img.numpy().flatten())  # flatten 3D tensor to 1D vector\n        labels.append(label)\n    X = np.array(images).T  # shape: (features, samples)\n    Y = np.array(labels).reshape(1, -1)  # shape: (1, samples)\n    return X, Y\n\n# Convert datasets to NumPy\nx_train, y_train = subset_to_numpy(train_ts)\nx_test, y_test = subset_to_numpy(test_ts)\n\n# Call your logistic regression function\naccuracy_log=logistic_regression(x_train, y_train, x_test, y_test, learning_rate=0.01, num_iterations=1400)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T13:12:07.319347Z","iopub.execute_input":"2025-05-08T13:12:07.319599Z","iopub.status.idle":"2025-05-08T13:20:01.379709Z","shell.execute_reply.started":"2025-05-08T13:12:07.319580Z","shell.execute_reply":"2025-05-08T13:20:01.378694Z"}},"outputs":[{"name":"stdout","text":"Cost after iteration 0: 5.936325\nCost after iteration 100: 0.540683\nCost after iteration 200: 0.479720\nCost after iteration 300: 0.444329\nCost after iteration 400: 0.420768\nCost after iteration 500: 0.403783\nCost after iteration 600: 0.390880\nCost after iteration 700: 0.380702\nCost after iteration 800: 0.372433\nCost after iteration 900: 0.365557\nCost after iteration 1000: 0.359725\nCost after iteration 1100: 0.354698\nCost after iteration 1200: 0.350303\nCost after iteration 1300: 0.346412\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Test Accuracy: 83.87 %\nTrain Accuracy: 84.27 %\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n# Reuse conversion function\ndef dataset_to_numpy(dataset):\n    X = []\n    y = []\n    for img, label in dataset:\n        X.append(img.view(-1).numpy())  # Flatten image\n        y.append(label)\n    return np.array(X), np.array(y)\n\n# Prepare data\nX_train, y_train = dataset_to_numpy(train_ts)\nX_val, y_val = dataset_to_numpy(val_ts)\nX_test, y_test = dataset_to_numpy(test_ts)\n# Combine training and validation sets\nX_train_final = np.concatenate((X_train, X_val))\ny_train_final = np.concatenate((y_train, y_val))\n\n# Retrain with best parameters\nfinal_model_log = LogisticRegression(\n    C=10,\n    penalty='l2',\n    solver='lbfgs',\n    max_iter=1500\n)\nfinal_model_log.fit(X_train_final, y_train_final)\n\n# Predict on test set\ny_pred_test = final_model_log.predict(X_test)\n\n# Evaluate\ntest_accuracy = accuracy_score(y_test, y_pred_test)\nprint(f\"âœ… Final Test Accuracy: {test_accuracy * 100:.2f}%\\n\")\naccuracy_log=test_accuracy\n# Optional: Detailed classification report\nprint(\"ðŸ“Š Classification Report:\")\nprint(classification_report(y_test, y_pred_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T13:20:01.380876Z","iopub.execute_input":"2025-05-08T13:20:01.381275Z","iopub.status.idle":"2025-05-08T13:22:51.093618Z","shell.execute_reply.started":"2025-05-08T13:20:01.381235Z","shell.execute_reply":"2025-05-08T13:22:51.092847Z"}},"outputs":[{"name":"stdout","text":"âœ… Final Test Accuracy: 91.62%\n\nðŸ“Š Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.88      0.91      0.90      1309\n           1       0.94      0.92      0.93      1971\n\n    accuracy                           0.92      3280\n   macro avg       0.91      0.92      0.91      3280\nweighted avg       0.92      0.92      0.92      3280\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=64, shuffle=False)\nimport numpy as np\n\ndef predict_unlabeled_sklearn(model, loader):\n    model_preds = []\n\n    for X_batch in loader:\n        # Se il loader restituisce tuple, prendi solo le immagini\n        if isinstance(X_batch, (list, tuple)):\n            X_batch = X_batch[0]\n\n        # Converti da Tensor a NumPy\n        X_np = X_batch.numpy()  # shape: (B, 3, 32, 32)\n\n        # Flatten per Logistic Regression: (B, 3*32*32)\n        X_np = X_np.reshape(X_np.shape[0], -1)\n\n        preds = model.predict(X_np)\n        model_preds.extend(preds)\n\n    return model_preds\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T13:22:51.094759Z","iopub.execute_input":"2025-05-08T13:22:51.095026Z","iopub.status.idle":"2025-05-08T13:22:51.102390Z","shell.execute_reply.started":"2025-05-08T13:22:51.095005Z","shell.execute_reply":"2025-05-08T13:22:51.101281Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score\nfrom torchvision.models import resnet18\nimport torch, torch.nn as nn\nfrom torch.utils.data import Subset, DataLoader\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 1ï¸âƒ£  Loader per train + val (indici train_idx) e per test (giÃ  definito)\ntrain_val_dataset = Subset(img_dataset, train_idx)\ntrain_val_loader  = DataLoader(train_val_dataset, batch_size=64, shuffle=True)\n\n# se non hai giÃ  test_loader:\ntest_loader = DataLoader(test_ts, batch_size=64, shuffle=False)\n\n# 2ï¸âƒ£  ResNet18 senza pesi preaddestrati â€“ iperparametri: lr=0.001, opt=Adam\nfinal_model_Res = resnet18(weights=None)\nnum_classes = len({lbl for _, lbl in train_val_dataset})\nfinal_model_Res.fc = nn.Linear(final_model_Res.fc.in_features, num_classes)\nfinal_model_Res.to(device)\n\ncriterion  = nn.CrossEntropyLoss()\noptimizer  = torch.optim.SGD(final_model_Res.parameters(), lr=0.001, momentum=0.9)\nnum_epochs = 15\n\n# 3ï¸âƒ£  Training su train + val con print di progresso\nfor epoch in range(num_epochs):\n    final_model_Res.train()\n    running_loss = 0.0\n    total_batches = len(train_val_loader)\n\n    print(f\"\\nðŸŸ¢ Epoch {epoch+1}/{num_epochs} - Training...\")\n\n    for batch_idx, (imgs, labels) in enumerate(train_val_loader):\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = final_model_Res(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == total_batches:\n            print(f\"  ðŸ” Batch {batch_idx + 1}/{total_batches} - Loss: {loss.item():.4f}\")\n\n    avg_loss = running_loss / total_batches\n    print(f\"âœ… Epoch {epoch+1} completed - Avg Loss: {avg_loss:.4f}\")\n\n# 4ï¸âƒ£  Valutazione sul test set\ny_true, y_pred = [], []\nfinal_model_Res.eval()\nwith torch.no_grad():\n    for imgs, labels in test_loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        _, preds = torch.max(final_model_Res(imgs), 1)\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(preds.cpu().numpy())\n\nprint(\"ðŸŽ¯ Test Accuracy:\", accuracy_score(y_true, y_pred))\nprint(classification_report(y_true, y_pred, digits=4))\naccuracy_Res=accuracy_score(y_true, y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T13:22:51.103423Z","iopub.execute_input":"2025-05-08T13:22:51.103670Z","iopub.status.idle":"2025-05-08T13:58:04.972329Z","shell.execute_reply.started":"2025-05-08T13:22:51.103651Z","shell.execute_reply":"2025-05-08T13:58:04.971156Z"}},"outputs":[{"name":"stdout","text":"\nðŸŸ¢ Epoch 1/15 - Training...\n  ðŸ” Batch 10/240 - Loss: 3.3768\n  ðŸ” Batch 20/240 - Loss: 0.4896\n  ðŸ” Batch 30/240 - Loss: 0.1164\n  ðŸ” Batch 40/240 - Loss: 0.1034\n  ðŸ” Batch 50/240 - Loss: 0.1496\n  ðŸ” Batch 60/240 - Loss: 0.1525\n  ðŸ” Batch 70/240 - Loss: 0.3921\n  ðŸ” Batch 80/240 - Loss: 0.3161\n  ðŸ” Batch 90/240 - Loss: 0.1472\n  ðŸ” Batch 100/240 - Loss: 0.0486\n  ðŸ” Batch 110/240 - Loss: 0.1719\n  ðŸ” Batch 120/240 - Loss: 0.1585\n  ðŸ” Batch 130/240 - Loss: 0.0786\n  ðŸ” Batch 140/240 - Loss: 0.1114\n  ðŸ” Batch 150/240 - Loss: 0.1885\n  ðŸ” Batch 160/240 - Loss: 0.0484\n  ðŸ” Batch 170/240 - Loss: 0.0802\n  ðŸ” Batch 180/240 - Loss: 0.1761\n  ðŸ” Batch 190/240 - Loss: 0.0540\n  ðŸ” Batch 200/240 - Loss: 0.0854\n  ðŸ” Batch 210/240 - Loss: 0.2683\n  ðŸ” Batch 220/240 - Loss: 0.1350\n  ðŸ” Batch 230/240 - Loss: 0.1239\n  ðŸ” Batch 240/240 - Loss: 0.1027\nâœ… Epoch 1 completed - Avg Loss: 0.4864\n\nðŸŸ¢ Epoch 2/15 - Training...\n  ðŸ” Batch 10/240 - Loss: 0.1256\n  ðŸ” Batch 20/240 - Loss: 0.0157\n  ðŸ” Batch 30/240 - Loss: 0.2197\n  ðŸ” Batch 40/240 - Loss: 0.0437\n  ðŸ” Batch 50/240 - Loss: 0.0715\n  ðŸ” Batch 60/240 - Loss: 0.0876\n  ðŸ” Batch 70/240 - Loss: 0.0406\n  ðŸ” Batch 80/240 - Loss: 0.0360\n  ðŸ” Batch 90/240 - Loss: 0.0245\n  ðŸ” Batch 100/240 - Loss: 0.1595\n  ðŸ” Batch 110/240 - Loss: 0.0485\n  ðŸ” Batch 120/240 - Loss: 0.1113\n  ðŸ” Batch 130/240 - Loss: 0.0955\n  ðŸ” Batch 140/240 - Loss: 0.1100\n  ðŸ” Batch 150/240 - Loss: 0.0196\n  ðŸ” Batch 160/240 - Loss: 0.0281\n  ðŸ” Batch 170/240 - Loss: 0.0111\n  ðŸ” Batch 180/240 - Loss: 0.0305\n  ðŸ” Batch 190/240 - Loss: 0.1222\n  ðŸ” Batch 200/240 - Loss: 0.0183\n  ðŸ” Batch 210/240 - Loss: 0.0945\n  ðŸ” Batch 220/240 - Loss: 0.0419\n  ðŸ” Batch 230/240 - Loss: 0.0451\n  ðŸ” Batch 240/240 - Loss: 0.0102\nâœ… Epoch 2 completed - Avg Loss: 0.0751\n\nðŸŸ¢ Epoch 3/15 - Training...\n  ðŸ” Batch 10/240 - Loss: 0.0652\n  ðŸ” Batch 20/240 - Loss: 0.0552\n  ðŸ” Batch 30/240 - Loss: 0.0268\n  ðŸ” Batch 40/240 - Loss: 0.0226\n  ðŸ” Batch 50/240 - Loss: 0.0145\n  ðŸ” Batch 60/240 - Loss: 0.0580\n  ðŸ” Batch 70/240 - Loss: 0.0140\n  ðŸ” Batch 80/240 - Loss: 0.0582\n  ðŸ” Batch 90/240 - Loss: 0.1286\n  ðŸ” Batch 100/240 - Loss: 0.1140\n  ðŸ” Batch 110/240 - Loss: 0.1150\n  ðŸ” Batch 120/240 - Loss: 0.0451\n  ðŸ” Batch 130/240 - Loss: 0.0343\n  ðŸ” Batch 140/240 - Loss: 0.0676\n  ðŸ” Batch 150/240 - Loss: 0.0162\n  ðŸ” Batch 160/240 - Loss: 0.0327\n  ðŸ” Batch 170/240 - Loss: 0.0311\n  ðŸ” Batch 180/240 - Loss: 0.0516\n  ðŸ” Batch 190/240 - Loss: 0.0235\n  ðŸ” Batch 200/240 - Loss: 0.0177\n  ðŸ” Batch 210/240 - Loss: 0.0720\n  ðŸ” Batch 220/240 - Loss: 0.0693\n  ðŸ” Batch 230/240 - Loss: 0.0359\n  ðŸ” Batch 240/240 - Loss: 0.0252\nâœ… Epoch 3 completed - Avg Loss: 0.0536\n\nðŸŸ¢ Epoch 4/15 - Training...\n  ðŸ” Batch 10/240 - Loss: 0.0630\n  ðŸ” Batch 20/240 - Loss: 0.0087\n  ðŸ” Batch 30/240 - Loss: 0.0085\n  ðŸ” Batch 40/240 - Loss: 0.0119\n  ðŸ” Batch 50/240 - Loss: 0.0174\n  ðŸ” Batch 60/240 - Loss: 0.0966\n  ðŸ” Batch 70/240 - Loss: 0.0657\n  ðŸ” Batch 80/240 - Loss: 0.0876\n  ðŸ” Batch 90/240 - Loss: 0.0105\n  ðŸ” Batch 100/240 - Loss: 0.0098\n  ðŸ” Batch 110/240 - Loss: 0.0218\n  ðŸ” Batch 120/240 - Loss: 0.0621\n  ðŸ” Batch 130/240 - Loss: 0.0139\n  ðŸ” Batch 140/240 - Loss: 0.0543\n  ðŸ” Batch 150/240 - Loss: 0.1181\n  ðŸ” Batch 160/240 - Loss: 0.0187\n  ðŸ” Batch 170/240 - Loss: 0.0039\n  ðŸ” Batch 180/240 - Loss: 0.0187\n  ðŸ” Batch 190/240 - Loss: 0.0415\n  ðŸ” Batch 200/240 - Loss: 0.0101\n  ðŸ” Batch 210/240 - Loss: 0.0127\n  ðŸ” Batch 220/240 - Loss: 0.0828\n  ðŸ” Batch 230/240 - Loss: 0.1138\n  ðŸ” Batch 240/240 - Loss: 0.1358\nâœ… Epoch 4 completed - Avg Loss: 0.0353\n\nðŸŸ¢ Epoch 5/15 - Training...\n  ðŸ” Batch 10/240 - Loss: 0.0914\n  ðŸ” Batch 20/240 - Loss: 0.0201\n  ðŸ” Batch 30/240 - Loss: 0.0091\n  ðŸ” Batch 40/240 - Loss: 0.0327\n  ðŸ” Batch 50/240 - Loss: 0.0030\n  ðŸ” Batch 60/240 - Loss: 0.0529\n  ðŸ” Batch 70/240 - Loss: 0.0044\n  ðŸ” Batch 80/240 - Loss: 0.0165\n  ðŸ” Batch 90/240 - Loss: 0.0375\n  ðŸ” Batch 100/240 - Loss: 0.0039\n  ðŸ” Batch 110/240 - Loss: 0.0016\n  ðŸ” Batch 120/240 - Loss: 0.0039\n  ðŸ” Batch 130/240 - Loss: 0.0521\n  ðŸ” Batch 140/240 - Loss: 0.0087\n  ðŸ” Batch 150/240 - Loss: 0.0131\n  ðŸ” Batch 160/240 - Loss: 0.0056\n  ðŸ” Batch 170/240 - Loss: 0.0060\n  ðŸ” Batch 180/240 - Loss: 0.0043\n  ðŸ” Batch 190/240 - Loss: 0.0605\n  ðŸ” Batch 200/240 - Loss: 0.0020\n  ðŸ” Batch 210/240 - Loss: 0.0148\n  ðŸ” Batch 220/240 - Loss: 0.0218\n  ðŸ” Batch 230/240 - Loss: 0.0037\n  ðŸ” Batch 240/240 - Loss: 0.0023\nâœ… Epoch 5 completed - Avg Loss: 0.0238\n\nðŸŸ¢ Epoch 6/15 - Training...\n  ðŸ” Batch 10/240 - Loss: 0.0081\n  ðŸ” Batch 20/240 - Loss: 0.0053\n  ðŸ” Batch 30/240 - Loss: 0.0215\n  ðŸ” Batch 40/240 - Loss: 0.0019\n  ðŸ” Batch 50/240 - Loss: 0.0019\n  ðŸ” Batch 60/240 - Loss: 0.1065\n  ðŸ” Batch 70/240 - Loss: 0.0476\n  ðŸ” Batch 80/240 - Loss: 0.0034\n  ðŸ” Batch 90/240 - Loss: 0.0555\n  ðŸ” Batch 100/240 - Loss: 0.0074\n  ðŸ” Batch 110/240 - Loss: 0.0211\n  ðŸ” Batch 120/240 - Loss: 0.0056\n  ðŸ” Batch 130/240 - Loss: 0.0083\n  ðŸ” Batch 140/240 - Loss: 0.0139\n  ðŸ” Batch 150/240 - Loss: 0.0009\n  ðŸ” Batch 160/240 - Loss: 0.0123\n  ðŸ” Batch 170/240 - Loss: 0.0476\n  ðŸ” Batch 180/240 - Loss: 0.0257\n  ðŸ” Batch 190/240 - Loss: 0.1151\n  ðŸ” Batch 200/240 - Loss: 0.0164\n  ðŸ” Batch 210/240 - Loss: 0.0397\n  ðŸ” Batch 220/240 - Loss: 0.0237\n  ðŸ” Batch 230/240 - Loss: 0.0051\n  ðŸ” Batch 240/240 - Loss: 0.0013\nâœ… Epoch 6 completed - Avg Loss: 0.0240\n\nðŸŸ¢ Epoch 7/15 - Training...\n  ðŸ” Batch 10/240 - Loss: 0.0056\n  ðŸ” Batch 20/240 - Loss: 0.0058\n  ðŸ” Batch 30/240 - Loss: 0.0099\n  ðŸ” Batch 40/240 - Loss: 0.0121\n  ðŸ” Batch 50/240 - Loss: 0.0028\n  ðŸ” Batch 60/240 - Loss: 0.0030\n  ðŸ” Batch 70/240 - Loss: 0.0047\n  ðŸ” Batch 80/240 - Loss: 0.0918\n  ðŸ” Batch 90/240 - Loss: 0.0045\n  ðŸ” Batch 100/240 - Loss: 0.0138\n  ðŸ” Batch 110/240 - Loss: 0.0096\n  ðŸ” Batch 120/240 - Loss: 0.0232\n  ðŸ” Batch 130/240 - Loss: 0.0031\n  ðŸ” Batch 140/240 - Loss: 0.0012\n  ðŸ” Batch 150/240 - Loss: 0.0099\n  ðŸ” Batch 160/240 - Loss: 0.0075\n  ðŸ” Batch 170/240 - Loss: 0.0257\n  ðŸ” Batch 180/240 - Loss: 0.0020\n  ðŸ” Batch 190/240 - Loss: 0.0043\n  ðŸ” Batch 200/240 - Loss: 0.0332\n  ðŸ” Batch 210/240 - Loss: 0.0125\n  ðŸ” Batch 220/240 - Loss: 0.0056\n  ðŸ” Batch 230/240 - Loss: 0.0131\n  ðŸ” Batch 240/240 - Loss: 1.3090\nâœ… Epoch 7 completed - Avg Loss: 0.0190\n\nðŸŸ¢ Epoch 8/15 - Training...\n  ðŸ” Batch 10/240 - Loss: 0.2057\n  ðŸ” Batch 20/240 - Loss: 0.1856\n  ðŸ” Batch 30/240 - Loss: 0.0501\n  ðŸ” Batch 40/240 - Loss: 0.0990\n  ðŸ” Batch 50/240 - Loss: 0.0725\n  ðŸ” Batch 60/240 - Loss: 0.0027\n  ðŸ” Batch 70/240 - Loss: 0.0133\n  ðŸ” Batch 80/240 - Loss: 0.1316\n  ðŸ” Batch 90/240 - Loss: 0.0076\n  ðŸ” Batch 100/240 - Loss: 0.0324\n  ðŸ” Batch 110/240 - Loss: 0.0459\n  ðŸ” Batch 120/240 - Loss: 0.0076\n  ðŸ” Batch 130/240 - Loss: 0.0052\n  ðŸ” Batch 140/240 - Loss: 0.0141\n  ðŸ” Batch 150/240 - Loss: 0.0860\n  ðŸ” Batch 160/240 - Loss: 0.0141\n  ðŸ” Batch 170/240 - Loss: 0.0221\n  ðŸ” Batch 180/240 - Loss: 0.0108\n  ðŸ” Batch 190/240 - Loss: 0.0986\n  ðŸ” Batch 200/240 - Loss: 0.0160\n  ðŸ” Batch 210/240 - Loss: 0.0066\n  ðŸ” Batch 220/240 - Loss: 0.0062\n  ðŸ” Batch 230/240 - Loss: 0.0736\n  ðŸ” Batch 240/240 - Loss: 1.2473\nâœ… Epoch 8 completed - Avg Loss: 0.0450\n\nðŸŸ¢ Epoch 9/15 - Training...\n  ðŸ” Batch 10/240 - Loss: 0.0861\n  ðŸ” Batch 20/240 - Loss: 0.1886\n  ðŸ” Batch 30/240 - Loss: 0.0454\n  ðŸ” Batch 40/240 - Loss: 0.0354\n  ðŸ” Batch 50/240 - Loss: 0.0279\n  ðŸ” Batch 60/240 - Loss: 0.0310\n  ðŸ” Batch 70/240 - Loss: 0.0129\n  ðŸ” Batch 80/240 - Loss: 0.0092\n  ðŸ” Batch 90/240 - Loss: 0.0154\n  ðŸ” Batch 100/240 - Loss: 0.0051\n  ðŸ” Batch 110/240 - Loss: 0.0035\n  ðŸ” Batch 120/240 - Loss: 0.0197\n  ðŸ” Batch 130/240 - Loss: 0.0057\n  ðŸ” Batch 140/240 - Loss: 0.0133\n  ðŸ” Batch 150/240 - Loss: 0.1298\n  ðŸ” Batch 160/240 - Loss: 0.0042\n  ðŸ” Batch 170/240 - Loss: 0.0039\n  ðŸ” Batch 180/240 - Loss: 0.0047\n  ðŸ” Batch 190/240 - Loss: 0.0284\n  ðŸ” Batch 200/240 - Loss: 0.0182\n  ðŸ” Batch 210/240 - Loss: 0.0029\n  ðŸ” Batch 220/240 - Loss: 0.0051\n  ðŸ” Batch 230/240 - Loss: 0.0847\n  ðŸ” Batch 240/240 - Loss: 0.0000\nâœ… Epoch 9 completed - Avg Loss: 0.0275\n\nðŸŸ¢ Epoch 10/15 - Training...\n  ðŸ” Batch 10/240 - Loss: 0.0064\n  ðŸ” Batch 20/240 - Loss: 0.0071\n  ðŸ” Batch 30/240 - Loss: 0.0253\n  ðŸ” Batch 40/240 - Loss: 0.0012\n  ðŸ” Batch 50/240 - Loss: 0.0078\n  ðŸ” Batch 60/240 - Loss: 0.0130\n  ðŸ” Batch 70/240 - Loss: 0.0163\n  ðŸ” Batch 80/240 - Loss: 0.0121\n  ðŸ” Batch 90/240 - Loss: 0.0032\n  ðŸ” Batch 100/240 - Loss: 0.0122\n  ðŸ” Batch 110/240 - Loss: 0.0016\n  ðŸ” Batch 120/240 - Loss: 0.0024\n  ðŸ” Batch 130/240 - Loss: 0.0035\n  ðŸ” Batch 140/240 - Loss: 0.0020\n  ðŸ” Batch 150/240 - Loss: 0.0002\n  ðŸ” Batch 160/240 - Loss: 0.0004\n  ðŸ” Batch 170/240 - Loss: 0.0049\n  ðŸ” Batch 180/240 - Loss: 0.0018\n  ðŸ” Batch 190/240 - Loss: 0.0016\n  ðŸ” Batch 200/240 - Loss: 0.0567\n  ðŸ” Batch 210/240 - Loss: 0.0136\n  ðŸ” Batch 220/240 - Loss: 0.0146\n  ðŸ” Batch 230/240 - Loss: 0.0049\n  ðŸ” Batch 240/240 - Loss: 0.0002\nâœ… Epoch 10 completed - Avg Loss: 0.0108\n\nðŸŸ¢ Epoch 11/15 - Training...\n  ðŸ” Batch 10/240 - Loss: 0.0027\n  ðŸ” Batch 20/240 - Loss: 0.0013\n  ðŸ” Batch 30/240 - Loss: 0.0019\n  ðŸ” Batch 40/240 - Loss: 0.0022\n  ðŸ” Batch 50/240 - Loss: 0.0019\n  ðŸ” Batch 60/240 - Loss: 0.0040\n  ðŸ” Batch 70/240 - Loss: 0.0017\n  ðŸ” Batch 80/240 - Loss: 0.0015\n  ðŸ” Batch 90/240 - Loss: 0.0003\n  ðŸ” Batch 100/240 - Loss: 0.0029\n  ðŸ” Batch 110/240 - Loss: 0.0026\n  ðŸ” Batch 120/240 - Loss: 0.0018\n  ðŸ” Batch 130/240 - Loss: 0.0004\n  ðŸ” Batch 140/240 - Loss: 0.0012\n  ðŸ” Batch 150/240 - Loss: 0.0006\n  ðŸ” Batch 160/240 - Loss: 0.0090\n  ðŸ” Batch 170/240 - Loss: 0.0012\n  ðŸ” Batch 180/240 - Loss: 0.0012\n  ðŸ” Batch 190/240 - Loss: 0.0016\n  ðŸ” Batch 200/240 - Loss: 0.0004\n  ðŸ” Batch 210/240 - Loss: 0.0003\n  ðŸ” Batch 220/240 - Loss: 0.0011\n  ðŸ” Batch 230/240 - Loss: 0.0050\n  ðŸ” Batch 240/240 - Loss: 0.5200\nâœ… Epoch 11 completed - Avg Loss: 0.0073\n\nðŸŸ¢ Epoch 12/15 - Training...\n  ðŸ” Batch 10/240 - Loss: 0.0657\n  ðŸ” Batch 20/240 - Loss: 0.0170\n  ðŸ” Batch 30/240 - Loss: 0.0179\n  ðŸ” Batch 40/240 - Loss: 0.0715\n  ðŸ” Batch 50/240 - Loss: 0.0058\n  ðŸ” Batch 60/240 - Loss: 0.0028\n  ðŸ” Batch 70/240 - Loss: 0.0013\n  ðŸ” Batch 80/240 - Loss: 0.0161\n  ðŸ” Batch 90/240 - Loss: 0.0205\n  ðŸ” Batch 100/240 - Loss: 0.1003\n  ðŸ” Batch 110/240 - Loss: 0.0134\n  ðŸ” Batch 120/240 - Loss: 0.0069\n  ðŸ” Batch 130/240 - Loss: 0.0029\n  ðŸ” Batch 140/240 - Loss: 0.0557\n  ðŸ” Batch 150/240 - Loss: 0.0005\n  ðŸ” Batch 160/240 - Loss: 0.0024\n  ðŸ” Batch 170/240 - Loss: 0.0237\n  ðŸ” Batch 180/240 - Loss: 0.0032\n  ðŸ” Batch 190/240 - Loss: 0.0038\n  ðŸ” Batch 200/240 - Loss: 0.0138\n  ðŸ” Batch 210/240 - Loss: 0.0156\n  ðŸ” Batch 220/240 - Loss: 0.0021\n  ðŸ” Batch 230/240 - Loss: 0.0080\n  ðŸ” Batch 240/240 - Loss: 0.8868\nâœ… Epoch 12 completed - Avg Loss: 0.0211\n\nðŸŸ¢ Epoch 13/15 - Training...\n  ðŸ” Batch 10/240 - Loss: 0.0144\n  ðŸ” Batch 20/240 - Loss: 0.0292\n  ðŸ” Batch 30/240 - Loss: 0.0219\n  ðŸ” Batch 40/240 - Loss: 0.0062\n  ðŸ” Batch 50/240 - Loss: 0.0136\n  ðŸ” Batch 60/240 - Loss: 0.0533\n  ðŸ” Batch 70/240 - Loss: 0.0037\n  ðŸ” Batch 80/240 - Loss: 0.1272\n  ðŸ” Batch 90/240 - Loss: 0.0018\n  ðŸ” Batch 100/240 - Loss: 0.0136\n  ðŸ” Batch 110/240 - Loss: 0.0083\n  ðŸ” Batch 120/240 - Loss: 0.0024\n  ðŸ” Batch 130/240 - Loss: 0.0024\n  ðŸ” Batch 140/240 - Loss: 0.0053\n  ðŸ” Batch 150/240 - Loss: 0.0042\n  ðŸ” Batch 160/240 - Loss: 0.0069\n  ðŸ” Batch 170/240 - Loss: 0.0098\n  ðŸ” Batch 180/240 - Loss: 0.0163\n  ðŸ” Batch 190/240 - Loss: 0.0098\n  ðŸ” Batch 200/240 - Loss: 0.0081\n  ðŸ” Batch 210/240 - Loss: 0.0146\n  ðŸ” Batch 220/240 - Loss: 0.0057\n  ðŸ” Batch 230/240 - Loss: 0.0002\n  ðŸ” Batch 240/240 - Loss: 0.0176\nâœ… Epoch 13 completed - Avg Loss: 0.0191\n\nðŸŸ¢ Epoch 14/15 - Training...\n  ðŸ” Batch 10/240 - Loss: 0.0009\n  ðŸ” Batch 20/240 - Loss: 0.0591\n  ðŸ” Batch 30/240 - Loss: 0.0039\n  ðŸ” Batch 40/240 - Loss: 0.0265\n  ðŸ” Batch 50/240 - Loss: 0.0019\n  ðŸ” Batch 60/240 - Loss: 0.0199\n  ðŸ” Batch 70/240 - Loss: 0.0155\n  ðŸ” Batch 80/240 - Loss: 0.0033\n  ðŸ” Batch 90/240 - Loss: 0.0057\n  ðŸ” Batch 100/240 - Loss: 0.0016\n  ðŸ” Batch 110/240 - Loss: 0.0009\n  ðŸ” Batch 120/240 - Loss: 0.0004\n  ðŸ” Batch 130/240 - Loss: 0.0010\n  ðŸ” Batch 140/240 - Loss: 0.0010\n  ðŸ” Batch 150/240 - Loss: 0.0109\n  ðŸ” Batch 160/240 - Loss: 0.0045\n  ðŸ” Batch 170/240 - Loss: 0.0047\n  ðŸ” Batch 180/240 - Loss: 0.0042\n  ðŸ” Batch 190/240 - Loss: 0.0023\n  ðŸ” Batch 200/240 - Loss: 0.0015\n  ðŸ” Batch 210/240 - Loss: 0.0001\n  ðŸ” Batch 220/240 - Loss: 0.0037\n  ðŸ” Batch 230/240 - Loss: 0.0029\n  ðŸ” Batch 240/240 - Loss: 0.0123\nâœ… Epoch 14 completed - Avg Loss: 0.0057\n\nðŸŸ¢ Epoch 15/15 - Training...\n  ðŸ” Batch 10/240 - Loss: 0.0020\n  ðŸ” Batch 20/240 - Loss: 0.0005\n  ðŸ” Batch 30/240 - Loss: 0.0398\n  ðŸ” Batch 40/240 - Loss: 0.0014\n  ðŸ” Batch 50/240 - Loss: 0.0002\n  ðŸ” Batch 60/240 - Loss: 0.0004\n  ðŸ” Batch 70/240 - Loss: 0.0010\n  ðŸ” Batch 80/240 - Loss: 0.0001\n  ðŸ” Batch 90/240 - Loss: 0.0009\n  ðŸ” Batch 100/240 - Loss: 0.0020\n  ðŸ” Batch 110/240 - Loss: 0.0284\n  ðŸ” Batch 120/240 - Loss: 0.0096\n  ðŸ” Batch 130/240 - Loss: 0.0002\n  ðŸ” Batch 140/240 - Loss: 0.0076\n  ðŸ” Batch 150/240 - Loss: 0.0002\n  ðŸ” Batch 160/240 - Loss: 0.0024\n  ðŸ” Batch 170/240 - Loss: 0.0002\n  ðŸ” Batch 180/240 - Loss: 0.0021\n  ðŸ” Batch 190/240 - Loss: 0.0012\n  ðŸ” Batch 200/240 - Loss: 0.0015\n  ðŸ” Batch 210/240 - Loss: 0.0038\n  ðŸ” Batch 220/240 - Loss: 0.0021\n  ðŸ” Batch 230/240 - Loss: 0.0075\n  ðŸ” Batch 240/240 - Loss: 0.0177\nâœ… Epoch 15 completed - Avg Loss: 0.0057\nðŸŽ¯ Test Accuracy: 0.9128048780487805\n              precision    recall  f1-score   support\n\n           0     0.8211    0.9992    0.9014      1309\n           1     0.9994    0.8554    0.9218      1971\n\n    accuracy                         0.9128      3280\n   macro avg     0.9102    0.9273    0.9116      3280\nweighted avg     0.9282    0.9128    0.9137      3280\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\nimport numpy as np\nimport torch\n\n# Funzione per convertire un dataset PyTorch in NumPy\ndef dataset_to_numpy(dataset):\n    X = []\n    y = []\n    for img, label in dataset:\n        X.append(img.view(-1).numpy())  # Flatten image (3x32x32 â†’ 3072)\n        y.append(label)\n    return np.array(X), np.array(y)\n\n# Converti i dataset\nX_train, y_train = dataset_to_numpy(train_ts)\nX_val, y_val     = dataset_to_numpy(val_ts)\nX_test, y_test   = dataset_to_numpy(test_ts)\n\n# Normalizza\nX_train = X_train / 255.0\nX_val   = X_val / 255.0\nX_test  = X_test / 255.0\n\n# Combina train + val per addestrare il modello finale\nX_combined = np.concatenate((X_train, X_val))\ny_combined = np.concatenate((y_train, y_val))\n\n# ðŸ”§ Crea e addestra il modello con i best params\nfinal_model_SVM = SVC(C=10, kernel='rbf', gamma='scale')\nfinal_model_SVM.fit(X_combined, y_combined)\n\n# ðŸ” Valutazione sul test set\ntest_preds = final_model_SVM.predict(X_test)\naccuracy_SVM = accuracy_score(y_test, test_preds)\n\nprint(f\"\\nðŸŽ¯ Test Accuracy: {accuracy_SVM * 100:.2f}%\")\nprint(classification_report(y_test, test_preds, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T13:58:04.973428Z","iopub.execute_input":"2025-05-08T13:58:04.973775Z","iopub.status.idle":"2025-05-08T14:11:12.521618Z","shell.execute_reply.started":"2025-05-08T13:58:04.973753Z","shell.execute_reply":"2025-05-08T14:11:12.520480Z"}},"outputs":[{"name":"stdout","text":"\nðŸŽ¯ Test Accuracy: 95.88%\n              precision    recall  f1-score   support\n\n           0     0.9407    0.9572    0.9489      1309\n           1     0.9713    0.9599    0.9656      1971\n\n    accuracy                         0.9588      3280\n   macro avg     0.9560    0.9586    0.9572      3280\nweighted avg     0.9591    0.9588    0.9589      3280\n\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"logreg_preds = predict_unlabeled_sklearn(final_model_log, unlabeled_loader)\nsvm_predictions = predict_unlabeled_sklearn(final_model_SVM, unlabeled_loader)\ncnn_predictions = predict_unlabeled(final_model_CNN, unlabeled_loader)\nres_predictions= predict_unlabeled(final_model_Res, unlabeled_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:26:20.035936Z","iopub.execute_input":"2025-05-08T14:26:20.036786Z","iopub.status.idle":"2025-05-08T14:28:07.332832Z","shell.execute_reply.started":"2025-05-08T14:26:20.036750Z","shell.execute_reply":"2025-05-08T14:28:07.331722Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Calcola i pesi (proporzionali all'accuracy)\ntotal = accuracy_log + accuracy_SVM + accuracy_CNN + accuracy_Res\nweights = [\n    accuracy_log / total,\n    accuracy_SVM    / total,\n    accuracy_CNN    / total,\n    accuracy_Res / total\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:13:32.409350Z","iopub.execute_input":"2025-05-08T14:13:32.409687Z","iopub.status.idle":"2025-05-08T14:13:32.415370Z","shell.execute_reply.started":"2025-05-08T14:13:32.409664Z","shell.execute_reply":"2025-05-08T14:13:32.414164Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Calcola il voto pesato per ogni immagine\nfinal_predictions = []\n\nfor lr, svm, cnn, res in zip(logreg_preds, svm_predictions, cnn_predictions, res_predictions):\n    weighted_vote = (\n        weights[0] * lr +\n        weights[1] * svm +\n        weights[2] * cnn +\n        weights[3] * res\n    )\n    final_label = 1 if weighted_vote >= 0.5 else 0\n    final_predictions.append(final_label)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:23:18.165266Z","iopub.execute_input":"2025-05-08T14:23:18.165602Z","iopub.status.idle":"2025-05-08T14:23:18.178092Z","shell.execute_reply.started":"2025-05-08T14:23:18.165580Z","shell.execute_reply":"2025-05-08T14:23:18.176597Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Estrai i nomi dei file dal dataset\nimage_ids = [os.path.splitext(os.path.basename(p))[0] for p in unlabeled_dataset.image_paths]\n\nsubmission_df = pd.DataFrame({\n    'id': image_ids,  # giÃ  ottenuti prima da unlabeled_dataset\n    'label': final_predictions\n})\n\nsubmission_df.to_csv('ensemble_predictions.csv', index=False)\nprint(\"âœ… File 'ensemble_predictions.csv' salvato con successo.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:24:58.375601Z","iopub.execute_input":"2025-05-08T14:24:58.376617Z","iopub.status.idle":"2025-05-08T14:24:58.398167Z","shell.execute_reply.started":"2025-05-08T14:24:58.376593Z","shell.execute_reply":"2025-05-08T14:24:58.396874Z"}},"outputs":[{"name":"stdout","text":"âœ… File 'ensemble_predictions.csv' salvato con successo.\n","output_type":"stream"}],"execution_count":29}]}